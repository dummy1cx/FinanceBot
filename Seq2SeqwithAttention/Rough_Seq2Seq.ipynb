{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iV6w5ZI4ZgvA"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import csv\n",
        "\n",
        "def extract_instruction_pairs(json_path):\n",
        "    with open(json_path, 'r', encoding='utf-8') as f:\n",
        "        data = json.load(f)\n",
        "\n",
        "    pairs = []\n",
        "\n",
        "    for item in data:\n",
        "        instruction = item.get(\"instruction\", \"\").strip()\n",
        "        input_text = item.get(\"input\", \"\").strip()\n",
        "        output_text = item.get(\"output\", \"\").strip()\n",
        "\n",
        "        if instruction and input_text:\n",
        "            prompt = f\"{instruction}\\n{input_text}\"\n",
        "        else:\n",
        "            prompt = instruction or input_text\n",
        "\n",
        "        if prompt and output_text:\n",
        "            pairs.append([prompt, output_text])\n",
        "\n",
        "    return pairs\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "json_path = \"/content/Cleaned_date.json\"\n",
        "pairs = extract_instruction_pairs(json_path)\n",
        "\n",
        "print(f\"Extracted {len(pairs)} pairs\")\n",
        "print(pairs[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fy7PqHfgZ15l",
        "outputId": "5381c120-3092-450c-d963-a17c76f79f80"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted 68911 pairs\n",
            "['For a car, what scams can be plotted with 0% financing vs rebate?', \"The car deal makes money 3 ways. If you pay in one lump payment. If the payment is greater than what they paid for the car, plus their expenses, they make a profit. They loan you the money. You make payments over months or years, if the total amount you pay is greater than what they paid for the car, plus their expenses, plus their finance expenses they make money. Of course the money takes years to come in, or they sell your loan to another business to get the money faster but in a smaller amount. You trade in a car and they sell it at a profit. Of course that new transaction could be a lump sum or a loan on the used car... They or course make money if you bring the car back for maintenance, or you buy lots of expensive dealer options. Some dealers wave two deals in front of you: get a 0% interest loan. These tend to be shorter 12 months vs 36,48,60 or even 72 months. The shorter length makes it harder for many to afford. If you can't swing the 12 large payments they offer you at x% loan for y years that keeps the payments in your budget. pay cash and get a rebate. If you take the rebate you can't get the 0% loan. If you take the 0% loan you can't get the rebate. The price you negotiate minus the rebate is enough to make a profit. The key is not letting them know which offer you are interested in. Don't even mention a trade in until the price of the new car has been finalized. Otherwise they will adjust the price, rebate, interest rate, length of loan,  and trade-in value to maximize their profit. The suggestion of running the numbers through a spreadsheet is a good one. If you get a loan for 2% from your bank/credit union for 3 years and the rebate from the dealer, it will cost less in total than the 0% loan from the dealer. The key is to get the loan approved by the bank/credit union before meeting with the dealer. The money from the bank looks like cash to the dealer.\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_pairs_to_tsv(pairs, save_path):\n",
        "    with open(save_path, 'w', encoding='utf-8', newline='') as f:\n",
        "        writer = csv.writer(f, delimiter='\\t')\n",
        "        for pair in pairs:\n",
        "            writer.writerow(pair)\n"
      ],
      "metadata": {
        "id": "7YIfAbPZcNHm"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_pairs_to_tsv(pairs, \"/content/formatted_pairs.txt\")\n",
        "print(\"OK!!!!Saved formatted pairs to /content/formatted_pairs.txt\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jrIf6kYNcqKs",
        "outputId": "e5cdfaab-e26b-4ac9-a5f7-7888590454eb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK!!!!Saved formatted pairs to /content/formatted_pairs.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import itertools\n",
        "import torch\n",
        "\n",
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count default tokens\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = [k for k, v in self.word2count.items() if v >= min_count]\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for seq in l:\n",
        "        m.append([0 if token == PAD_token else 1 for token in seq])\n",
        "    return m\n",
        "\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = zip(*pair_batch)\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len"
      ],
      "metadata": {
        "id": "sjqcL4SycqM6"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from utils import Voc, normalizeString, batch2TrainData\n",
        "\n",
        "MAX_LENGTH = 30\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, pairs, voc):\n",
        "        self.pairs = pairs\n",
        "        self.voc = voc\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]\n",
        "\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "    pairs = []\n",
        "    for l in lines:\n",
        "\n",
        "      parts = l.split('\\t')\n",
        "      if len(parts) == 2:\n",
        "\n",
        "        pairs.append([normalizeString(parts[0]), normalizeString(parts[1])])\n",
        "\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    voc.trim(MIN_COUNT)\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence, output_sentence = pair\n",
        "        keep_input = all(word in voc.word2index for word in input_sentence.split(' '))\n",
        "        keep_output = all(word in voc.word2index for word in output_sentence.split(' '))\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "def collate_fn(batch, voc):\n",
        "    return batch2TrainData(voc, batch)\n",
        "\n",
        "\n",
        "def split_dataset(pairs, test_size=0.1, random_state=42):\n",
        "    train_pairs, val_pairs = train_test_split(pairs, test_size=test_size, random_state=random_state)\n",
        "    print(f\"Split {len(pairs)} pairs into {len(train_pairs)} train and {len(val_pairs)} validation\")\n",
        "    return train_pairs, val_pairs\n",
        "\n",
        "\n",
        "def get_dataloader(pairs, voc, batch_size=64, shuffle=True):\n",
        "    dataset = ChatDataset(pairs, voc)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=lambda x: collate_fn(x, voc))"
      ],
      "metadata": {
        "id": "FuzQ8lODcqPQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Encoder RNN\n",
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers,\n",
        "                          dropout=(0 if n_layers == 1 else dropout),\n",
        "                          bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths)\n",
        "        outputs, hidden = self.gru(packed, hidden)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "        return outputs, hidden\n",
        "\n",
        "\n",
        "# Attention mechanism\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(self.method, \"is not an appropriate attention method.\")\n",
        "\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1), encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'dot':\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        attn_energies = attn_energies.t()\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "\n",
        "# Decoder with Luong Attention\n",
        "class LuongAttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LuongAttnDecoderRNN, self).__init__()\n",
        "\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.embedding = embedding\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size, n_layers, dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        rnn_output, hidden = self.gru(embedded, last_hidden)\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        return output, hidden"
      ],
      "metadata": {
        "id": "OyqmVZ2ncqRV"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import wandb\n",
        "\n",
        "def init_wandb(project_name=\"seq2seqRNNXAtten_last\", config=None):\n",
        "    \"\"\"\n",
        "    Initializes wandb api key = \"dadc5760c5180b0aa661c9e66b78c7e8af724486\"\n",
        "    \"\"\"\n",
        "    if config is None:\n",
        "        config = {\n",
        "            \"model_name\": \"cb_model\",\n",
        "            \"attn_model\": \"dot\",\n",
        "            \"hidden_size\": 500,\n",
        "            \"encoder_n_layers\": 3,\n",
        "            \"decoder_n_layers\": 3,\n",
        "            \"dropout\": 0.2,\n",
        "            \"batch_size\": 64,\n",
        "            \"learning_rate\": 0.0001,\n",
        "            \"decoder_learning_ratio\": 5.0,\n",
        "            \"teacher_forcing_ratio\": 0.9,\n",
        "            \"clip\": 50.0,\n",
        "            \"n_iteration\": 4000,\n",
        "            \"print_every\": 10,\n",
        "            \"save_every\": 500\n",
        "        }\n",
        "\n",
        "    wandb.init(project=project_name, config=config)\n",
        "    return wandb.config\n"
      ],
      "metadata": {
        "id": "2XuxlUX2fvpj"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "#from utils import Voc\n",
        "#from dataset import loadPrepareData, trimRareWords, split_dataset, get_dataloader\n",
        "#from models_seq2seq import EncoderRNN, LuongAttnDecoderRNN\n",
        "#from wandb_config import init_wandb\n",
        "from datetime import datetime\n",
        "\n",
        "\n",
        "dev_config = init_wandb()\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "datafile = \"/content/formatted_pairs.txt\"\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "corpus_name = \"custom\"\n",
        "voc, pairs = loadPrepareData(\"data\", corpus_name, datafile, save_dir)\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT=dev_config.MIN_COUNT if \"MIN_COUNT\" in dev_config else 3)\n",
        "\n",
        "\n",
        "train_pairs, val_pairs = split_dataset(pairs, test_size=0.1)\n",
        "\n",
        "\n",
        "train_loader = get_dataloader(train_pairs, voc, batch_size=dev_config.batch_size)\n",
        "val_loader = get_dataloader(val_pairs, voc, batch_size=dev_config.batch_size)\n",
        "\n",
        "\n",
        "embedding = nn.Embedding(voc.num_words, dev_config.hidden_size)\n",
        "\n",
        "\n",
        "encoder = EncoderRNN(dev_config.hidden_size, embedding, dev_config.encoder_n_layers, dev_config.dropout).to(device)\n",
        "decoder = LuongAttnDecoderRNN(dev_config.attn_model, embedding, dev_config.hidden_size, voc.num_words,\n",
        "                               dev_config.decoder_n_layers, dev_config.dropout).to(device)\n",
        "\n",
        "\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=dev_config.learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=dev_config.learning_rate * dev_config.decoder_learning_ratio)\n",
        "\n",
        "def save_checkpoint_tar(voc, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, iteration, loss, save_path=\"checkpoint.tar\"):\n",
        "    checkpoint = {\n",
        "        'iteration': iteration,\n",
        "        'encoder_state': encoder.state_dict(),\n",
        "        'decoder_state': decoder.state_dict(),\n",
        "        'embedding_state': embedding.state_dict(),\n",
        "        'encoder_optimizer_state': encoder_optimizer.state_dict(),\n",
        "        'decoder_optimizer_state': decoder_optimizer.state_dict(),\n",
        "        'voc_dict': voc.__dict__,\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, save_path)\n",
        "\n",
        "    with open(\"voc.pkl\", \"wb\") as f:\n",
        "        pickle.dump(voc, f)\n",
        "\n",
        "def log_artifacts_to_wandb(tar_path=\"checkpoint.tar\", voc_path=\"voc.pkl\", artifact_name=\"chatbot_model\"):\n",
        "    artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
        "    artifact.add_file(tar_path)\n",
        "    artifact.add_file(voc_path)\n",
        "    wandb.log_artifact(artifact)\n",
        "\n",
        "\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    return loss, nTotal.item()\n",
        "\n",
        "\n",
        "def train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "          encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, clip):\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    current_batch_size = input_variable.size(1)\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "\n",
        "    decoder_input = torch.LongTensor([[1 for _ in range(current_batch_size)]]).to(device)\n",
        "    decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "    use_teacher_forcing = True if torch.rand(1).item() < dev_config.teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(current_batch_size)]]).to(device)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), dev_config.clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), dev_config.clip)\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals\n",
        "\n",
        "\n",
        "def evaluate_loss(val_loader, encoder, decoder, embedding):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    total_loss = 0\n",
        "    total_count = 0\n",
        "    with torch.no_grad():\n",
        "        for input_variable, lengths, target_variable, mask, max_target_len in val_loader:\n",
        "            input_variable = input_variable.to(device)\n",
        "            target_variable = target_variable.to(device)\n",
        "            mask = mask.to(device)\n",
        "            lengths = lengths.to(\"cpu\")\n",
        "\n",
        "            current_batch_size = input_variable.size(1)\n",
        "\n",
        "            encoder_outputs, encoder_hidden = encoder(input_variable, lengths)\n",
        "            decoder_input = torch.LongTensor([[1 for _ in range(current_batch_size)]]).to(device)\n",
        "            decoder_hidden = encoder_hidden[:decoder.n_layers]\n",
        "\n",
        "            for t in range(max_target_len):\n",
        "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                decoder_input = target_variable[t].view(1, -1)\n",
        "                mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "                total_loss += mask_loss.item() * nTotal\n",
        "                total_count += nTotal\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    return total_loss / total_count\n",
        "\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "train_iter = iter(train_loader)\n",
        "for iteration in range(1, dev_config.n_iteration + 1):\n",
        "    try:\n",
        "        batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        batch = next(train_iter)\n",
        "\n",
        "    input_variable, lengths, target_variable, mask, max_target_len = batch\n",
        "    train_loss = train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "                       encoder, decoder, embedding, encoder_optimizer, decoder_optimizer,\n",
        "                       dev_config.clip)\n",
        "\n",
        "    perplexity = math.exp(train_loss)\n",
        "    wandb.log({\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_perplexity\": perplexity,\n",
        "        \"iteration\": iteration\n",
        "    })\n",
        "\n",
        "    if iteration % dev_config.print_every == 0:\n",
        "        print(\"Iteration: {}; Train Loss: {:.4f} | Perplexity: {:.4f}\".format(iteration, train_loss, perplexity))\n",
        "\n",
        "    if iteration % dev_config.save_every == 0:\n",
        "        val_loss = evaluate_loss(val_loader, encoder, decoder, embedding)\n",
        "        val_perplexity = math.exp(val_loss)\n",
        "        wandb.log({\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_perplexity\": val_perplexity\n",
        "        })\n",
        "        print(\"Validation Loss: {:.4f} | Perplexity: {:.4f}\".format(val_loss, val_perplexity))\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        save_checkpoint_tar(voc, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, iteration, val_loss, f\"checkpoint_{timestamp}.tar\")\n",
        "        log_artifacts_to_wandb(f\"checkpoint_{timestamp}.tar\", \"voc.pkl\", f\"chatbot_checkpoint_{iteration}\")\n",
        "\n",
        "\n",
        "save_checkpoint_tar(voc, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, iteration, train_loss, \"final_checkpoint.tar\")\n",
        "log_artifacts_to_wandb(\"final_checkpoint.tar\", \"voc.pkl\", \"final_chatbot_checkpoint\")\n",
        "print(\"✅ Final checkpoint saved to W&B.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bF8YjkdfgEc9",
        "outputId": "b1e23be4-7dc3-4262-a1e3-2b14ca516e17"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▃▄▄▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇█</td></tr><tr><td>train_loss</td><td>██▇▇▇▆▆▆▅▅▆▅▆▅▅▅▄▄▄▃▄▃▃▃▃▃▂▂▂▂▃▃▂▂▂▂▂▂▁▁</td></tr><tr><td>train_perplexity</td><td>█▇▇█▇▆▆▄▄▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▂▁▁</td></tr><tr><td>val_perplexity</td><td>█▄▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iteration</td><td>3403</td></tr><tr><td>train_loss</td><td>2.78323</td></tr><tr><td>train_perplexity</td><td>16.17115</td></tr><tr><td>val_loss</td><td>4.76762</td></tr><tr><td>val_perplexity</td><td>117.63876</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">peach-spaceship-2</strong> at: <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqRNNXAtten_last/runs/b69vjhmk' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqRNNXAtten_last/runs/b69vjhmk</a><br> View project at: <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqRNNXAtten_last' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqRNNXAtten_last</a><br>Synced 5 W&B file(s), 0 media file(s), 12 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250417_144504-b69vjhmk/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250417_145433-u42a6ile</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqRNNXAtten_last/runs/u42a6ile' target=\"_blank\">crimson-fire-3</a></strong> to <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqRNNXAtten_last' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqRNNXAtten_last' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqRNNXAtten_last</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqRNNXAtten_last/runs/u42a6ile' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqRNNXAtten_last/runs/u42a6ile</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 69127 sentence pairs\n",
            "Trimmed to 31953 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 23180\n",
            "keep_words 10574 / 23177 = 0.4562\n",
            "Trimmed from 31953 pairs to 22654, 0.7090 of total\n",
            "Split 22654 pairs into 20388 train and 2266 validation\n",
            "\n",
            "Starting training...\n",
            "Iteration: 10; Train Loss: 7.6856 | Perplexity: 2176.7997\n",
            "Iteration: 20; Train Loss: 6.5281 | Perplexity: 684.1121\n",
            "Iteration: 30; Train Loss: 6.6333 | Perplexity: 760.0228\n",
            "Iteration: 40; Train Loss: 6.6907 | Perplexity: 804.8645\n",
            "Iteration: 50; Train Loss: 6.6562 | Perplexity: 777.5857\n",
            "Iteration: 60; Train Loss: 6.4715 | Perplexity: 646.4559\n",
            "Iteration: 70; Train Loss: 6.3146 | Perplexity: 552.6043\n",
            "Iteration: 80; Train Loss: 6.4418 | Perplexity: 627.5380\n",
            "Iteration: 90; Train Loss: 6.3352 | Perplexity: 564.0979\n",
            "Iteration: 100; Train Loss: 6.3868 | Perplexity: 593.9703\n",
            "Iteration: 110; Train Loss: 6.5949 | Perplexity: 731.3902\n",
            "Iteration: 120; Train Loss: 6.3684 | Perplexity: 583.1318\n",
            "Iteration: 130; Train Loss: 6.2428 | Perplexity: 514.2964\n",
            "Iteration: 140; Train Loss: 6.2162 | Perplexity: 500.7816\n",
            "Iteration: 150; Train Loss: 6.2684 | Perplexity: 527.6576\n",
            "Iteration: 160; Train Loss: 6.1712 | Perplexity: 478.7476\n",
            "Iteration: 170; Train Loss: 6.3545 | Perplexity: 575.0555\n",
            "Iteration: 180; Train Loss: 6.0120 | Perplexity: 408.2877\n",
            "Iteration: 190; Train Loss: 6.1756 | Perplexity: 480.8849\n",
            "Iteration: 200; Train Loss: 6.0675 | Perplexity: 431.5888\n",
            "Iteration: 210; Train Loss: 5.7278 | Perplexity: 307.2931\n",
            "Iteration: 220; Train Loss: 6.0220 | Perplexity: 412.4133\n",
            "Iteration: 230; Train Loss: 6.0989 | Perplexity: 445.3889\n",
            "Iteration: 240; Train Loss: 6.2128 | Perplexity: 499.0849\n",
            "Iteration: 250; Train Loss: 5.9031 | Perplexity: 366.1653\n",
            "Iteration: 260; Train Loss: 5.8546 | Perplexity: 348.8353\n",
            "Iteration: 270; Train Loss: 5.9059 | Perplexity: 367.2149\n",
            "Iteration: 280; Train Loss: 6.0010 | Perplexity: 403.8467\n",
            "Iteration: 290; Train Loss: 5.8117 | Perplexity: 334.1794\n",
            "Iteration: 300; Train Loss: 6.1729 | Perplexity: 479.5810\n",
            "Iteration: 310; Train Loss: 5.9110 | Perplexity: 369.0769\n",
            "Iteration: 320; Train Loss: 6.1094 | Perplexity: 450.0581\n",
            "Iteration: 330; Train Loss: 5.9970 | Perplexity: 402.2170\n",
            "Iteration: 340; Train Loss: 5.9737 | Perplexity: 392.9516\n",
            "Iteration: 350; Train Loss: 5.7811 | Perplexity: 324.1086\n",
            "Iteration: 360; Train Loss: 5.7356 | Perplexity: 309.6882\n",
            "Iteration: 370; Train Loss: 5.8521 | Perplexity: 347.9522\n",
            "Iteration: 380; Train Loss: 5.8397 | Perplexity: 343.6591\n",
            "Iteration: 390; Train Loss: 5.4561 | Perplexity: 234.1919\n",
            "Iteration: 400; Train Loss: 5.4066 | Perplexity: 222.8798\n",
            "Iteration: 410; Train Loss: 5.7276 | Perplexity: 307.2162\n",
            "Iteration: 420; Train Loss: 5.6377 | Perplexity: 280.8097\n",
            "Iteration: 430; Train Loss: 5.7724 | Perplexity: 321.3237\n",
            "Iteration: 440; Train Loss: 5.8189 | Perplexity: 336.6144\n",
            "Iteration: 450; Train Loss: 5.5037 | Perplexity: 245.6013\n",
            "Iteration: 460; Train Loss: 5.7705 | Perplexity: 320.7065\n",
            "Iteration: 470; Train Loss: 5.5516 | Perplexity: 257.6388\n",
            "Iteration: 480; Train Loss: 5.4326 | Perplexity: 228.7444\n",
            "Iteration: 490; Train Loss: 5.6253 | Perplexity: 277.3525\n",
            "Iteration: 500; Train Loss: 5.9500 | Perplexity: 383.7724\n",
            "Validation Loss: 5.6988 | Perplexity: 298.5046\n",
            "Iteration: 510; Train Loss: 6.3822 | Perplexity: 591.2300\n",
            "Iteration: 520; Train Loss: 5.5228 | Perplexity: 250.3325\n",
            "Iteration: 530; Train Loss: 5.5280 | Perplexity: 251.6299\n",
            "Iteration: 540; Train Loss: 5.6104 | Perplexity: 273.2503\n",
            "Iteration: 550; Train Loss: 5.8283 | Perplexity: 339.7717\n",
            "Iteration: 560; Train Loss: 5.3605 | Perplexity: 212.8282\n",
            "Iteration: 570; Train Loss: 5.6112 | Perplexity: 273.4702\n",
            "Iteration: 580; Train Loss: 5.6428 | Perplexity: 282.2528\n",
            "Iteration: 590; Train Loss: 6.0162 | Perplexity: 410.0261\n",
            "Iteration: 600; Train Loss: 5.4273 | Perplexity: 227.5324\n",
            "Iteration: 610; Train Loss: 5.5148 | Perplexity: 248.3389\n",
            "Iteration: 620; Train Loss: 5.5449 | Perplexity: 255.9338\n",
            "Iteration: 630; Train Loss: 6.2482 | Perplexity: 517.0573\n",
            "Iteration: 640; Train Loss: 5.4340 | Perplexity: 229.0660\n",
            "Iteration: 650; Train Loss: 5.1757 | Perplexity: 176.9151\n",
            "Iteration: 660; Train Loss: 5.4159 | Perplexity: 224.9525\n",
            "Iteration: 670; Train Loss: 5.2406 | Perplexity: 188.7783\n",
            "Iteration: 680; Train Loss: 5.4288 | Perplexity: 227.8743\n",
            "Iteration: 690; Train Loss: 4.9802 | Perplexity: 145.5107\n",
            "Iteration: 700; Train Loss: 5.2479 | Perplexity: 190.1666\n",
            "Iteration: 710; Train Loss: 5.0780 | Perplexity: 160.4508\n",
            "Iteration: 720; Train Loss: 5.6280 | Perplexity: 278.1013\n",
            "Iteration: 730; Train Loss: 5.2815 | Perplexity: 196.6697\n",
            "Iteration: 740; Train Loss: 5.1937 | Perplexity: 180.1404\n",
            "Iteration: 750; Train Loss: 5.3117 | Perplexity: 202.6956\n",
            "Iteration: 760; Train Loss: 5.2481 | Perplexity: 190.2028\n",
            "Iteration: 770; Train Loss: 5.6183 | Perplexity: 275.4207\n",
            "Iteration: 780; Train Loss: 5.1042 | Perplexity: 164.7066\n",
            "Iteration: 790; Train Loss: 5.8753 | Perplexity: 356.1445\n",
            "Iteration: 800; Train Loss: 5.1220 | Perplexity: 167.6690\n",
            "Iteration: 810; Train Loss: 5.8492 | Perplexity: 346.9566\n",
            "Iteration: 820; Train Loss: 5.3717 | Perplexity: 215.2193\n",
            "Iteration: 830; Train Loss: 5.3111 | Perplexity: 202.5735\n",
            "Iteration: 840; Train Loss: 5.1612 | Perplexity: 174.3712\n",
            "Iteration: 850; Train Loss: 5.2875 | Perplexity: 197.8542\n",
            "Iteration: 860; Train Loss: 5.2938 | Perplexity: 199.1058\n",
            "Iteration: 870; Train Loss: 5.2069 | Perplexity: 182.5316\n",
            "Iteration: 880; Train Loss: 5.1023 | Perplexity: 164.4045\n",
            "Iteration: 890; Train Loss: 5.1924 | Perplexity: 179.8992\n",
            "Iteration: 900; Train Loss: 5.2723 | Perplexity: 194.8707\n",
            "Iteration: 910; Train Loss: 5.4737 | Perplexity: 238.3325\n",
            "Iteration: 920; Train Loss: 5.2315 | Perplexity: 187.0760\n",
            "Iteration: 930; Train Loss: 5.1339 | Perplexity: 169.6858\n",
            "Iteration: 940; Train Loss: 5.1710 | Perplexity: 176.0844\n",
            "Iteration: 950; Train Loss: 5.1633 | Perplexity: 174.7320\n",
            "Iteration: 960; Train Loss: 5.8933 | Perplexity: 362.5933\n",
            "Iteration: 970; Train Loss: 4.9517 | Perplexity: 141.4135\n",
            "Iteration: 980; Train Loss: 5.0117 | Perplexity: 150.1575\n",
            "Iteration: 990; Train Loss: 5.0058 | Perplexity: 149.2817\n",
            "Iteration: 1000; Train Loss: 4.9281 | Perplexity: 138.1172\n",
            "Validation Loss: 5.3064 | Perplexity: 201.6214\n",
            "Iteration: 1010; Train Loss: 5.0615 | Perplexity: 157.8330\n",
            "Iteration: 1020; Train Loss: 5.2229 | Perplexity: 185.4764\n",
            "Iteration: 1030; Train Loss: 5.7342 | Perplexity: 309.2710\n",
            "Iteration: 1040; Train Loss: 4.8961 | Perplexity: 133.7706\n",
            "Iteration: 1050; Train Loss: 4.8427 | Perplexity: 126.8144\n",
            "Iteration: 1060; Train Loss: 4.9742 | Perplexity: 144.6384\n",
            "Iteration: 1070; Train Loss: 4.8566 | Perplexity: 128.5807\n",
            "Iteration: 1080; Train Loss: 5.7379 | Perplexity: 310.4043\n",
            "Iteration: 1090; Train Loss: 4.8506 | Perplexity: 127.8221\n",
            "Iteration: 1100; Train Loss: 4.8441 | Perplexity: 126.9943\n",
            "Iteration: 1110; Train Loss: 4.9023 | Perplexity: 134.6032\n",
            "Iteration: 1120; Train Loss: 4.8532 | Perplexity: 128.1535\n",
            "Iteration: 1130; Train Loss: 4.6559 | Perplexity: 105.2085\n",
            "Iteration: 1140; Train Loss: 5.0783 | Perplexity: 160.5054\n",
            "Iteration: 1150; Train Loss: 4.7885 | Perplexity: 120.1242\n",
            "Iteration: 1160; Train Loss: 4.9380 | Perplexity: 139.4854\n",
            "Iteration: 1170; Train Loss: 4.7306 | Perplexity: 113.3585\n",
            "Iteration: 1180; Train Loss: 5.0435 | Perplexity: 155.0147\n",
            "Iteration: 1190; Train Loss: 4.7709 | Perplexity: 118.0234\n",
            "Iteration: 1200; Train Loss: 4.8627 | Perplexity: 129.3723\n",
            "Iteration: 1210; Train Loss: 5.8139 | Perplexity: 334.9384\n",
            "Iteration: 1220; Train Loss: 5.1097 | Perplexity: 165.6131\n",
            "Iteration: 1230; Train Loss: 4.8221 | Perplexity: 124.2316\n",
            "Iteration: 1240; Train Loss: 4.8359 | Perplexity: 125.9502\n",
            "Iteration: 1250; Train Loss: 4.8479 | Perplexity: 127.4665\n",
            "Iteration: 1260; Train Loss: 4.9389 | Perplexity: 139.6149\n",
            "Iteration: 1270; Train Loss: 4.6783 | Perplexity: 107.5863\n",
            "Iteration: 1280; Train Loss: 4.4909 | Perplexity: 89.1985\n",
            "Iteration: 1290; Train Loss: 4.6655 | Perplexity: 106.2155\n",
            "Iteration: 1300; Train Loss: 4.5390 | Perplexity: 93.5952\n",
            "Iteration: 1310; Train Loss: 5.0012 | Perplexity: 148.5879\n",
            "Iteration: 1320; Train Loss: 4.4762 | Perplexity: 87.8967\n",
            "Iteration: 1330; Train Loss: 4.7563 | Perplexity: 116.3149\n",
            "Iteration: 1340; Train Loss: 4.9326 | Perplexity: 138.7335\n",
            "Iteration: 1350; Train Loss: 4.7246 | Perplexity: 112.6804\n",
            "Iteration: 1360; Train Loss: 4.5807 | Perplexity: 97.5782\n",
            "Iteration: 1370; Train Loss: 4.4307 | Perplexity: 83.9871\n",
            "Iteration: 1380; Train Loss: 4.8513 | Perplexity: 127.9021\n",
            "Iteration: 1390; Train Loss: 4.7146 | Perplexity: 111.5668\n",
            "Iteration: 1400; Train Loss: 4.5910 | Perplexity: 98.5947\n",
            "Iteration: 1410; Train Loss: 4.7179 | Perplexity: 111.9325\n",
            "Iteration: 1420; Train Loss: 5.6469 | Perplexity: 283.4234\n",
            "Iteration: 1430; Train Loss: 4.7295 | Perplexity: 113.2445\n",
            "Iteration: 1440; Train Loss: 5.5984 | Perplexity: 269.9813\n",
            "Iteration: 1450; Train Loss: 4.5860 | Perplexity: 98.1002\n",
            "Iteration: 1460; Train Loss: 4.6658 | Perplexity: 106.2485\n",
            "Iteration: 1470; Train Loss: 4.5758 | Perplexity: 97.1009\n",
            "Iteration: 1480; Train Loss: 4.3550 | Perplexity: 77.8649\n",
            "Iteration: 1490; Train Loss: 4.6781 | Perplexity: 107.5677\n",
            "Iteration: 1500; Train Loss: 5.4629 | Perplexity: 235.7836\n",
            "Validation Loss: 5.0707 | Perplexity: 159.2852\n",
            "Iteration: 1510; Train Loss: 4.8725 | Perplexity: 130.6474\n",
            "Iteration: 1520; Train Loss: 4.6051 | Perplexity: 99.9883\n",
            "Iteration: 1530; Train Loss: 4.5673 | Perplexity: 96.2831\n",
            "Iteration: 1540; Train Loss: 4.5553 | Perplexity: 95.1365\n",
            "Iteration: 1550; Train Loss: 4.4763 | Perplexity: 87.9110\n",
            "Iteration: 1560; Train Loss: 4.6799 | Perplexity: 107.7637\n",
            "Iteration: 1570; Train Loss: 4.3141 | Perplexity: 74.7431\n",
            "Iteration: 1580; Train Loss: 4.6912 | Perplexity: 108.9815\n",
            "Iteration: 1590; Train Loss: 4.4104 | Perplexity: 82.2984\n",
            "Iteration: 1600; Train Loss: 4.2893 | Perplexity: 72.9153\n",
            "Iteration: 1610; Train Loss: 4.3908 | Perplexity: 80.7025\n",
            "Iteration: 1620; Train Loss: 4.0435 | Perplexity: 57.0279\n",
            "Iteration: 1630; Train Loss: 4.3163 | Perplexity: 74.9107\n",
            "Iteration: 1640; Train Loss: 4.5214 | Perplexity: 91.9687\n",
            "Iteration: 1650; Train Loss: 4.1278 | Perplexity: 62.0413\n",
            "Iteration: 1660; Train Loss: 4.3326 | Perplexity: 76.1415\n",
            "Iteration: 1670; Train Loss: 4.3125 | Perplexity: 74.6263\n",
            "Iteration: 1680; Train Loss: 4.2201 | Perplexity: 68.0373\n",
            "Iteration: 1690; Train Loss: 4.4647 | Perplexity: 86.8913\n",
            "Iteration: 1700; Train Loss: 4.3980 | Perplexity: 81.2904\n",
            "Iteration: 1710; Train Loss: 4.3302 | Perplexity: 75.9570\n",
            "Iteration: 1720; Train Loss: 4.1621 | Perplexity: 64.2046\n",
            "Iteration: 1730; Train Loss: 4.2585 | Perplexity: 70.7037\n",
            "Iteration: 1740; Train Loss: 4.4997 | Perplexity: 89.9877\n",
            "Iteration: 1750; Train Loss: 4.3761 | Perplexity: 79.5312\n",
            "Iteration: 1760; Train Loss: 4.3694 | Perplexity: 78.9923\n",
            "Iteration: 1770; Train Loss: 4.3455 | Perplexity: 77.1340\n",
            "Iteration: 1780; Train Loss: 4.5280 | Perplexity: 92.5708\n",
            "Iteration: 1790; Train Loss: 4.3054 | Perplexity: 74.0958\n",
            "Iteration: 1800; Train Loss: 4.1370 | Perplexity: 62.6154\n",
            "Iteration: 1810; Train Loss: 4.3357 | Perplexity: 76.3771\n",
            "Iteration: 1820; Train Loss: 4.3494 | Perplexity: 77.4285\n",
            "Iteration: 1830; Train Loss: 4.5069 | Perplexity: 90.6378\n",
            "Iteration: 1840; Train Loss: 4.2131 | Perplexity: 67.5660\n",
            "Iteration: 1850; Train Loss: 4.2136 | Perplexity: 67.6015\n",
            "Iteration: 1860; Train Loss: 4.1735 | Perplexity: 64.9408\n",
            "Iteration: 1870; Train Loss: 4.1511 | Perplexity: 63.5068\n",
            "Iteration: 1880; Train Loss: 4.2762 | Perplexity: 71.9645\n",
            "Iteration: 1890; Train Loss: 4.2807 | Perplexity: 72.2909\n",
            "Iteration: 1900; Train Loss: 4.3702 | Perplexity: 79.0566\n",
            "Iteration: 1910; Train Loss: 4.2842 | Perplexity: 72.5473\n",
            "Iteration: 1920; Train Loss: 4.1951 | Perplexity: 66.3624\n",
            "Iteration: 1930; Train Loss: 4.4924 | Perplexity: 89.3381\n",
            "Iteration: 1940; Train Loss: 4.1393 | Perplexity: 62.7565\n",
            "Iteration: 1950; Train Loss: 4.1202 | Perplexity: 61.5712\n",
            "Iteration: 1960; Train Loss: 4.0813 | Perplexity: 59.2242\n",
            "Iteration: 1970; Train Loss: 3.9696 | Perplexity: 52.9612\n",
            "Iteration: 1980; Train Loss: 4.0536 | Perplexity: 57.6071\n",
            "Iteration: 1990; Train Loss: 4.1063 | Perplexity: 60.7237\n",
            "Iteration: 2000; Train Loss: 3.9879 | Perplexity: 53.9436\n",
            "Validation Loss: 4.9217 | Perplexity: 137.2320\n",
            "Iteration: 2010; Train Loss: 3.9316 | Perplexity: 50.9867\n",
            "Iteration: 2020; Train Loss: 5.4938 | Perplexity: 243.1797\n",
            "Iteration: 2030; Train Loss: 3.7892 | Perplexity: 44.2224\n",
            "Iteration: 2040; Train Loss: 4.0814 | Perplexity: 59.2263\n",
            "Iteration: 2050; Train Loss: 4.2602 | Perplexity: 70.8212\n",
            "Iteration: 2060; Train Loss: 5.4622 | Perplexity: 235.6127\n",
            "Iteration: 2070; Train Loss: 3.9214 | Perplexity: 50.4721\n",
            "Iteration: 2080; Train Loss: 4.1909 | Perplexity: 66.0850\n",
            "Iteration: 2090; Train Loss: 3.8519 | Perplexity: 47.0822\n",
            "Iteration: 2100; Train Loss: 4.3227 | Perplexity: 75.3949\n",
            "Iteration: 2110; Train Loss: 3.9091 | Perplexity: 49.8540\n",
            "Iteration: 2120; Train Loss: 4.2748 | Perplexity: 71.8654\n",
            "Iteration: 2130; Train Loss: 4.0236 | Perplexity: 55.9030\n",
            "Iteration: 2140; Train Loss: 4.0611 | Perplexity: 58.0387\n",
            "Iteration: 2150; Train Loss: 4.3021 | Perplexity: 73.8525\n",
            "Iteration: 2160; Train Loss: 4.0718 | Perplexity: 58.6597\n",
            "Iteration: 2170; Train Loss: 4.1367 | Perplexity: 62.5958\n",
            "Iteration: 2180; Train Loss: 4.1231 | Perplexity: 61.7521\n",
            "Iteration: 2190; Train Loss: 5.5035 | Perplexity: 245.5466\n",
            "Iteration: 2200; Train Loss: 5.3448 | Perplexity: 209.5221\n",
            "Iteration: 2210; Train Loss: 4.0149 | Perplexity: 55.4160\n",
            "Iteration: 2220; Train Loss: 3.8883 | Perplexity: 48.8292\n",
            "Iteration: 2230; Train Loss: 3.8730 | Perplexity: 48.0877\n",
            "Iteration: 2240; Train Loss: 5.1489 | Perplexity: 172.2368\n",
            "Iteration: 2250; Train Loss: 3.8151 | Perplexity: 45.3793\n",
            "Iteration: 2260; Train Loss: 3.8628 | Perplexity: 47.5961\n",
            "Iteration: 2270; Train Loss: 3.6421 | Perplexity: 38.1731\n",
            "Iteration: 2280; Train Loss: 3.8832 | Perplexity: 48.5803\n",
            "Iteration: 2290; Train Loss: 3.9079 | Perplexity: 49.7943\n",
            "Iteration: 2300; Train Loss: 3.9215 | Perplexity: 50.4773\n",
            "Iteration: 2310; Train Loss: 3.7730 | Perplexity: 43.5124\n",
            "Iteration: 2320; Train Loss: 3.7666 | Perplexity: 43.2340\n",
            "Iteration: 2330; Train Loss: 3.9036 | Perplexity: 49.5785\n",
            "Iteration: 2340; Train Loss: 4.1431 | Perplexity: 62.9955\n",
            "Iteration: 2350; Train Loss: 3.6362 | Perplexity: 37.9460\n",
            "Iteration: 2360; Train Loss: 3.8450 | Perplexity: 46.7581\n",
            "Iteration: 2370; Train Loss: 3.8047 | Perplexity: 44.9103\n",
            "Iteration: 2380; Train Loss: 3.8822 | Perplexity: 48.5324\n",
            "Iteration: 2390; Train Loss: 3.9426 | Perplexity: 51.5503\n",
            "Iteration: 2400; Train Loss: 3.8627 | Perplexity: 47.5931\n",
            "Iteration: 2410; Train Loss: 3.7852 | Perplexity: 44.0461\n",
            "Iteration: 2420; Train Loss: 4.0593 | Perplexity: 57.9334\n",
            "Iteration: 2430; Train Loss: 4.8680 | Perplexity: 130.0613\n",
            "Iteration: 2440; Train Loss: 4.0223 | Perplexity: 55.8279\n",
            "Iteration: 2450; Train Loss: 3.9550 | Perplexity: 52.1939\n",
            "Iteration: 2460; Train Loss: 4.0405 | Perplexity: 56.8572\n",
            "Iteration: 2470; Train Loss: 3.6904 | Perplexity: 40.0602\n",
            "Iteration: 2480; Train Loss: 3.7373 | Perplexity: 41.9842\n",
            "Iteration: 2490; Train Loss: 3.7312 | Perplexity: 41.7295\n",
            "Iteration: 2500; Train Loss: 3.8519 | Perplexity: 47.0843\n",
            "Validation Loss: 4.8579 | Perplexity: 128.7526\n",
            "Iteration: 2510; Train Loss: 5.5049 | Perplexity: 245.8861\n",
            "Iteration: 2520; Train Loss: 3.8799 | Perplexity: 48.4216\n",
            "Iteration: 2530; Train Loss: 4.2120 | Perplexity: 67.4940\n",
            "Iteration: 2540; Train Loss: 4.0573 | Perplexity: 57.8180\n",
            "Iteration: 2550; Train Loss: 3.7893 | Perplexity: 44.2261\n",
            "Iteration: 2560; Train Loss: 4.8312 | Perplexity: 125.3603\n",
            "Iteration: 2570; Train Loss: 3.6646 | Perplexity: 39.0418\n",
            "Iteration: 2580; Train Loss: 3.8282 | Perplexity: 45.9808\n",
            "Iteration: 2590; Train Loss: 3.5326 | Perplexity: 34.2141\n",
            "Iteration: 2600; Train Loss: 3.5159 | Perplexity: 33.6470\n",
            "Iteration: 2610; Train Loss: 3.6042 | Perplexity: 36.7507\n",
            "Iteration: 2620; Train Loss: 3.6990 | Perplexity: 40.4089\n",
            "Iteration: 2630; Train Loss: 3.5295 | Perplexity: 34.1078\n",
            "Iteration: 2640; Train Loss: 3.5152 | Perplexity: 33.6219\n",
            "Iteration: 2650; Train Loss: 3.7101 | Perplexity: 40.8594\n",
            "Iteration: 2660; Train Loss: 3.7224 | Perplexity: 41.3634\n",
            "Iteration: 2670; Train Loss: 3.7320 | Perplexity: 41.7611\n",
            "Iteration: 2680; Train Loss: 3.7023 | Perplexity: 40.5395\n",
            "Iteration: 2690; Train Loss: 5.1979 | Perplexity: 180.8847\n",
            "Iteration: 2700; Train Loss: 3.5414 | Perplexity: 34.5168\n",
            "Iteration: 2710; Train Loss: 3.7572 | Perplexity: 42.8304\n",
            "Iteration: 2720; Train Loss: 3.6317 | Perplexity: 37.7776\n",
            "Iteration: 2730; Train Loss: 3.6388 | Perplexity: 38.0471\n",
            "Iteration: 2740; Train Loss: 3.8007 | Perplexity: 44.7337\n",
            "Iteration: 2750; Train Loss: 3.8249 | Perplexity: 45.8288\n",
            "Iteration: 2760; Train Loss: 3.9241 | Perplexity: 50.6064\n",
            "Iteration: 2770; Train Loss: 3.6532 | Perplexity: 38.5990\n",
            "Iteration: 2780; Train Loss: 3.6980 | Perplexity: 40.3669\n",
            "Iteration: 2790; Train Loss: 3.7021 | Perplexity: 40.5307\n",
            "Iteration: 2800; Train Loss: 3.8139 | Perplexity: 45.3285\n",
            "Iteration: 2810; Train Loss: 3.9189 | Perplexity: 50.3475\n",
            "Iteration: 2820; Train Loss: 3.7870 | Perplexity: 44.1260\n",
            "Iteration: 2830; Train Loss: 3.4191 | Perplexity: 30.5430\n",
            "Iteration: 2840; Train Loss: 3.6555 | Perplexity: 38.6866\n",
            "Iteration: 2850; Train Loss: 3.7872 | Perplexity: 44.1313\n",
            "Iteration: 2860; Train Loss: 4.0032 | Perplexity: 54.7724\n",
            "Iteration: 2870; Train Loss: 3.7865 | Perplexity: 44.1003\n",
            "Iteration: 2880; Train Loss: 3.1912 | Perplexity: 24.3173\n",
            "Iteration: 2890; Train Loss: 3.3987 | Perplexity: 29.9239\n",
            "Iteration: 2900; Train Loss: 3.3994 | Perplexity: 29.9460\n",
            "Iteration: 2910; Train Loss: 3.5429 | Perplexity: 34.5670\n",
            "Iteration: 2920; Train Loss: 3.5294 | Perplexity: 34.1038\n",
            "Iteration: 2930; Train Loss: 3.4314 | Perplexity: 30.9200\n",
            "Iteration: 2940; Train Loss: 3.6297 | Perplexity: 37.7006\n",
            "Iteration: 2950; Train Loss: 3.4591 | Perplexity: 31.7879\n",
            "Iteration: 2960; Train Loss: 3.4719 | Perplexity: 32.1985\n",
            "Iteration: 2970; Train Loss: 3.5146 | Perplexity: 33.6022\n",
            "Iteration: 2980; Train Loss: 3.4867 | Perplexity: 32.6788\n",
            "Iteration: 2990; Train Loss: 4.8920 | Perplexity: 133.2163\n",
            "Iteration: 3000; Train Loss: 3.7301 | Perplexity: 41.6841\n",
            "Validation Loss: 4.8334 | Perplexity: 125.6399\n",
            "Iteration: 3010; Train Loss: 3.2989 | Perplexity: 27.0826\n",
            "Iteration: 3020; Train Loss: 3.7693 | Perplexity: 43.3514\n",
            "Iteration: 3030; Train Loss: 3.3601 | Perplexity: 28.7914\n",
            "Iteration: 3040; Train Loss: 3.7616 | Perplexity: 43.0178\n",
            "Iteration: 3050; Train Loss: 3.6115 | Perplexity: 37.0208\n",
            "Iteration: 3060; Train Loss: 3.6416 | Perplexity: 38.1517\n",
            "Iteration: 3070; Train Loss: 3.6298 | Perplexity: 37.7044\n",
            "Iteration: 3080; Train Loss: 5.0266 | Perplexity: 152.4121\n",
            "Iteration: 3090; Train Loss: 3.6819 | Perplexity: 39.7217\n",
            "Iteration: 3100; Train Loss: 3.5046 | Perplexity: 33.2667\n",
            "Iteration: 3110; Train Loss: 3.7131 | Perplexity: 40.9795\n",
            "Iteration: 3120; Train Loss: 3.5792 | Perplexity: 35.8466\n",
            "Iteration: 3130; Train Loss: 3.6459 | Perplexity: 38.3183\n",
            "Iteration: 3140; Train Loss: 3.1965 | Perplexity: 24.4471\n",
            "Iteration: 3150; Train Loss: 3.7015 | Perplexity: 40.5068\n",
            "Iteration: 3160; Train Loss: 3.4942 | Perplexity: 32.9250\n",
            "Iteration: 3170; Train Loss: 3.4082 | Perplexity: 30.2094\n",
            "Iteration: 3180; Train Loss: 3.6045 | Perplexity: 36.7637\n",
            "Iteration: 3190; Train Loss: 3.5563 | Perplexity: 35.0335\n",
            "Iteration: 3200; Train Loss: 5.1174 | Perplexity: 166.9030\n",
            "Iteration: 3210; Train Loss: 3.3828 | Perplexity: 29.4541\n",
            "Iteration: 3220; Train Loss: 3.4753 | Perplexity: 32.3075\n",
            "Iteration: 3230; Train Loss: 3.4749 | Perplexity: 32.2934\n",
            "Iteration: 3240; Train Loss: 3.2515 | Perplexity: 25.8288\n",
            "Iteration: 3250; Train Loss: 3.2402 | Perplexity: 25.5400\n",
            "Iteration: 3260; Train Loss: 3.2371 | Perplexity: 25.4600\n",
            "Iteration: 3270; Train Loss: 3.2813 | Perplexity: 26.6110\n",
            "Iteration: 3280; Train Loss: 3.4061 | Perplexity: 30.1477\n",
            "Iteration: 3290; Train Loss: 3.5378 | Perplexity: 34.3909\n",
            "Iteration: 3300; Train Loss: 3.2603 | Perplexity: 26.0562\n",
            "Iteration: 3310; Train Loss: 3.1908 | Perplexity: 24.3084\n",
            "Iteration: 3320; Train Loss: 3.3923 | Perplexity: 29.7356\n",
            "Iteration: 3330; Train Loss: 3.4904 | Perplexity: 32.7983\n",
            "Iteration: 3340; Train Loss: 3.3662 | Perplexity: 28.9677\n",
            "Iteration: 3350; Train Loss: 3.2678 | Perplexity: 26.2525\n",
            "Iteration: 3360; Train Loss: 3.6241 | Perplexity: 37.4916\n",
            "Iteration: 3370; Train Loss: 3.4746 | Perplexity: 32.2858\n",
            "Iteration: 3380; Train Loss: 3.6961 | Perplexity: 40.2882\n",
            "Iteration: 3390; Train Loss: 3.3508 | Perplexity: 28.5249\n",
            "Iteration: 3400; Train Loss: 3.4486 | Perplexity: 31.4549\n",
            "Iteration: 3410; Train Loss: 3.5782 | Perplexity: 35.8108\n",
            "Iteration: 3420; Train Loss: 3.2419 | Perplexity: 25.5814\n",
            "Iteration: 3430; Train Loss: 3.3227 | Perplexity: 27.7354\n",
            "Iteration: 3440; Train Loss: 3.4535 | Perplexity: 31.6097\n",
            "Iteration: 3450; Train Loss: 3.1109 | Perplexity: 22.4411\n",
            "Iteration: 3460; Train Loss: 3.3313 | Perplexity: 27.9746\n",
            "Iteration: 3470; Train Loss: 3.3692 | Perplexity: 29.0555\n",
            "Iteration: 3480; Train Loss: 4.9789 | Perplexity: 145.3107\n",
            "Iteration: 3490; Train Loss: 3.3952 | Perplexity: 29.8201\n",
            "Iteration: 3500; Train Loss: 5.1667 | Perplexity: 175.3331\n",
            "Validation Loss: 4.8065 | Perplexity: 122.3038\n",
            "Iteration: 3510; Train Loss: 3.0625 | Perplexity: 21.3799\n",
            "Iteration: 3520; Train Loss: 2.9347 | Perplexity: 18.8155\n",
            "Iteration: 3530; Train Loss: 3.1489 | Perplexity: 23.3093\n",
            "Iteration: 3540; Train Loss: 3.4313 | Perplexity: 30.9179\n",
            "Iteration: 3550; Train Loss: 3.1458 | Perplexity: 23.2384\n",
            "Iteration: 3560; Train Loss: 3.0308 | Perplexity: 20.7140\n",
            "Iteration: 3570; Train Loss: 3.1010 | Perplexity: 22.2193\n",
            "Iteration: 3580; Train Loss: 3.1296 | Perplexity: 22.8637\n",
            "Iteration: 3590; Train Loss: 3.2899 | Perplexity: 26.8390\n",
            "Iteration: 3600; Train Loss: 3.2070 | Perplexity: 24.7053\n",
            "Iteration: 3610; Train Loss: 3.1719 | Perplexity: 23.8519\n",
            "Iteration: 3620; Train Loss: 4.8457 | Perplexity: 127.1938\n",
            "Iteration: 3630; Train Loss: 3.1935 | Perplexity: 24.3745\n",
            "Iteration: 3640; Train Loss: 3.1992 | Perplexity: 24.5136\n",
            "Iteration: 3650; Train Loss: 3.3054 | Perplexity: 27.2602\n",
            "Iteration: 3660; Train Loss: 3.0804 | Perplexity: 21.7663\n",
            "Iteration: 3670; Train Loss: 5.0269 | Perplexity: 152.4672\n",
            "Iteration: 3680; Train Loss: 3.3027 | Perplexity: 27.1860\n",
            "Iteration: 3690; Train Loss: 3.2792 | Perplexity: 26.5548\n",
            "Iteration: 3700; Train Loss: 3.1778 | Perplexity: 23.9930\n",
            "Iteration: 3710; Train Loss: 3.3115 | Perplexity: 27.4270\n",
            "Iteration: 3720; Train Loss: 3.1519 | Perplexity: 23.3796\n",
            "Iteration: 3730; Train Loss: 3.0155 | Perplexity: 20.3996\n",
            "Iteration: 3740; Train Loss: 3.4080 | Perplexity: 30.2053\n",
            "Iteration: 3750; Train Loss: 3.1254 | Perplexity: 22.7681\n",
            "Iteration: 3760; Train Loss: 3.0488 | Perplexity: 21.0900\n",
            "Iteration: 3770; Train Loss: 3.3697 | Perplexity: 29.0701\n",
            "Iteration: 3780; Train Loss: 3.1155 | Perplexity: 22.5439\n",
            "Iteration: 3790; Train Loss: 3.2719 | Perplexity: 26.3610\n",
            "Iteration: 3800; Train Loss: 3.2705 | Perplexity: 26.3249\n",
            "Iteration: 3810; Train Loss: 4.6993 | Perplexity: 109.8756\n",
            "Iteration: 3820; Train Loss: 3.5472 | Perplexity: 34.7170\n",
            "Iteration: 3830; Train Loss: 3.0300 | Perplexity: 20.6971\n",
            "Iteration: 3840; Train Loss: 3.0320 | Perplexity: 20.7376\n",
            "Iteration: 3850; Train Loss: 2.9334 | Perplexity: 18.7914\n",
            "Iteration: 3860; Train Loss: 3.2153 | Perplexity: 24.9106\n",
            "Iteration: 3870; Train Loss: 3.0604 | Perplexity: 21.3358\n",
            "Iteration: 3880; Train Loss: 3.2087 | Perplexity: 24.7458\n",
            "Iteration: 3890; Train Loss: 2.8837 | Perplexity: 17.8801\n",
            "Iteration: 3900; Train Loss: 3.1404 | Perplexity: 23.1121\n",
            "Iteration: 3910; Train Loss: 3.0993 | Perplexity: 22.1824\n",
            "Iteration: 3920; Train Loss: 3.0656 | Perplexity: 21.4467\n",
            "Iteration: 3930; Train Loss: 2.9180 | Perplexity: 18.5043\n",
            "Iteration: 3940; Train Loss: 3.2260 | Perplexity: 25.1790\n",
            "Iteration: 3950; Train Loss: 3.0385 | Perplexity: 20.8732\n",
            "Iteration: 3960; Train Loss: 3.2117 | Perplexity: 24.8205\n",
            "Iteration: 3970; Train Loss: 4.8619 | Perplexity: 129.2752\n",
            "Iteration: 3980; Train Loss: 2.9350 | Perplexity: 18.8222\n",
            "Iteration: 3990; Train Loss: 2.8764 | Perplexity: 17.7511\n",
            "Iteration: 4000; Train Loss: 5.0388 | Perplexity: 154.2807\n",
            "Validation Loss: 4.8000 | Perplexity: 121.5111\n",
            "✅ Final checkpoint saved to W&B.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from utils import indexesFromSentence\n",
        "#from config import MAX_LENGTH  # if you have a shared config file\n",
        "\n",
        "def to_device(tensor):\n",
        "    return tensor.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "        decoder_hidden = encoder_hidden[:self.decoder.n_layers]\n",
        "        decoder_input = torch.ones(1, 1, device=input_seq.device, dtype=torch.long) * 1  # SOS_token\n",
        "\n",
        "        all_tokens = torch.zeros([0], device=input_seq.device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=input_seq.device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "\n",
        "        return all_tokens, all_scores\n",
        "\n",
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence)]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    input_batch = to_device(input_batch)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words\n"
      ],
      "metadata": {
        "id": "V9VLnR_An44-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "artifact = wandb.use_artifact(\"abhi1199-city-university-of-london/seq2seqRNNXAtten_last/final_chatbot_checkpoint:v0\", type=\"model\")\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "checkpoint_path = os.path.join(artifact_dir, \"final_checkpoint.tar\")\n",
        "voc_path = os.path.join(artifact_dir, \"voc.pkl\")\n",
        "\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCOfvPFLXg9-",
        "outputId": "731b7970-bc1e-4274-8ec1-978a3e39346f"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact final_chatbot_checkpoint:v0, 288.27MB. 2 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
            "Done. 0:0:2.8\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(voc_path, \"rb\") as f:\n",
        "    voc = pickle.load(f)\n"
      ],
      "metadata": {
        "id": "dW8xfNbZX2Wr"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "hidden_size = checkpoint['embedding_state']['weight'].shape[1]\n",
        "embedding = nn.Embedding(voc.num_words, hidden_size)\n",
        "embedding.load_state_dict(checkpoint['embedding_state'])\n",
        "embedding = embedding.to(device)\n",
        "\n",
        "encoder = EncoderRNN(hidden_size, embedding, n_layers=2, dropout=0.1).to(device)\n",
        "decoder = LuongAttnDecoderRNN(\"dot\", embedding, hidden_size, voc.num_words, n_layers=2, dropout=0.1).to(device)\n",
        "\n",
        "encoder.load_state_dict(checkpoint['encoder_state'])\n",
        "decoder.load_state_dict(checkpoint['decoder_state'])\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "\n",
        "\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n",
        "\n",
        "\n",
        "def chat():\n",
        "    print(\"Hello! Financeot is ready to solve your query! Type 'quit' to exit.\")\n",
        "    while True:\n",
        "        try:\n",
        "            input_sentence = input(\"> \")\n",
        "            if input_sentence.lower() in [\"quit\", \"q\"]:\n",
        "                break\n",
        "            input_sentence = normalizeString(input_sentence)\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            output_words = [w for w in output_words if w not in [\"EOS\", \"PAD\"]]\n",
        "            print(\"Bot:\", ' '.join(output_words))\n",
        "        except KeyError:\n",
        "            print(\"Oops! Encountered unknown word. Try again.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    chat()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OR1Hc7HDOhbi",
        "outputId": "963ee593-8894-4847-84f2-9a70cbee25f8"
      },
      "execution_count": 59,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello! Financeot is ready to solve your query! Type 'quit' to exit.\n",
            "> Why do some stocks have a higher margin requirement?\n",
            "Bot: some you can have a successful mobile of parts a test that can help you as much as possible . .\n",
            "> why do I need an emergency fund if I already have investments?\n",
            "Bot: i believe you believe and what you have that can have you been . it is always good to have have you to have . .\n",
            "> How would bonds fare if interest rates rose?\n",
            "Bot: there are three players you need to take a few steps . .\n",
            "> Simple and safe way to manage a lot of cash\n",
            "Bot: . what are the of you are ? .\n",
            "> Personal Tax Return software for Linux?\n",
            "Bot: the criteria for you for is as follows as of and and and and . and .\n",
            "> Is it possible to make money by getting a mortgage?\n",
            "Bot: it is not a common step to make it easy to pay . you are here to you pay about your money . you you you .\n",
            "> Is it ever a good idea to close credit cards?\n",
            "Bot: it is not a good step but you need to buy your budget but you can give it up . . you\n",
            "> Do I not have a credit score?\n",
            "Bot: i have not a few few but not not not . it does not have been very few . not not not . not .\n",
            "> What can cause rent prices to fall?\n",
            "Bot: . have a comprehensive type of reasons to the task or open a few situation . . . .\n",
            "> What is a good asset allocation for a 25 year old?\n",
            "Bot: . old to old old year old year . year year .\n",
            "> Summarize the functions of the Federal Reserve.\n",
            "Bot: the main of is a .\n",
            "> Create a container class that holds two values.\n",
            "Bot: class blue two two two two two two two ? two . a .\n",
            "> Write a short poem describing a setting sun.\n",
            "Bot: the sun sun sun\n",
            "> Generate a comment on a science article.\n",
            "Bot: the science of the science science the science . science . .\n",
            "> quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k3Np8s45Y1oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "why do I need an emergency fund if I already have investments?\n",
        "Why do some stocks have a higher margin requirement?\n"
      ],
      "metadata": {
        "id": "4eNAfbacYu_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "How would bonds fare if interest rates rose?"
      ],
      "metadata": {
        "id": "w6pMoSDGSvue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}