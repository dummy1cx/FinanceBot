{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iV6w5ZI4ZgvA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fy7PqHfgZ15l"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7YIfAbPZcNHm"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jrIf6kYNcqKs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import itertools\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count default tokens\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = [k for k, v in self.word2count.items() if v >= min_count]\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for seq in l:\n",
        "        m.append([0 if token == PAD_token else 1 for token in seq])\n",
        "    return m\n",
        "\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = zip(*pair_batch)\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "# GloVe integration\n",
        "def load_glove_embeddings(voc, glove_path, embedding_dim=100, freeze=False):\n",
        "    print(\"Loading GloVe embeddings...\")\n",
        "    glove = {}\n",
        "    with open(glove_path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            glove[word] = vector\n",
        "\n",
        "    embedding_matrix = np.random.normal(0, 1, (voc.num_words, embedding_dim)).astype(np.float32)\n",
        "    found = 0\n",
        "    for word, idx in voc.word2index.items():\n",
        "        if word in glove:\n",
        "            embedding_matrix[idx] = glove[word]\n",
        "            found += 1\n",
        "\n",
        "    print(f\"Found {found}/{voc.num_words} words in GloVe.\")\n",
        "    tensor = torch.tensor(embedding_matrix)\n",
        "    return nn.Embedding.from_pretrained(tensor, freeze=freeze)\n"
      ],
      "metadata": {
        "id": "sjqcL4SycqM6"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from utils import Voc, normalizeString, batch2TrainData, load_glove_embeddings\n",
        "\n",
        "MAX_LENGTH = 15\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, pairs, voc):\n",
        "        self.pairs = pairs\n",
        "        self.voc = voc\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]\n",
        "\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "    pairs = []\n",
        "    for l in lines:\n",
        "        parts = l.split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            pairs.append([normalizeString(parts[0]), normalizeString(parts[1])])\n",
        "\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    voc.trim(MIN_COUNT)\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence, output_sentence = pair\n",
        "        keep_input = all(word in voc.word2index for word in input_sentence.split(' '))\n",
        "        keep_output = all(word in voc.word2index for word in output_sentence.split(' '))\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "def collate_fn(batch, voc):\n",
        "    return batch2TrainData(voc, batch)\n",
        "\n",
        "def split_dataset(pairs, test_size=0.1, random_state=42):\n",
        "    train_pairs, val_pairs = train_test_split(pairs, test_size=test_size, random_state=random_state)\n",
        "    print(f\"Split {len(pairs)} pairs into {len(train_pairs)} train and {len(val_pairs)} validation\")\n",
        "    return train_pairs, val_pairs\n",
        "\n",
        "def get_dataloader(pairs, voc, batch_size=64, shuffle=True):\n",
        "    dataset = ChatDataset(pairs, voc)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=lambda x: collate_fn(x, voc))"
      ],
      "metadata": {
        "id": "FuzQ8lODcqPQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Encoder with LSTM\n",
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.lstm = nn.LSTM(embedding.embedding_dim, hidden_size, n_layers,\n",
        "                            dropout=(0 if n_layers == 1 else dropout),\n",
        "                            bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        self.lstm.flatten_parameters()  # Fix for RNN warning\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
        "        outputs, hidden = self.lstm(packed, hidden)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "        return outputs, hidden\n",
        "\n",
        "# Attention mechanism\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(f\"{self.method} is not a valid attention method.\")\n",
        "\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1),\n",
        "                                      encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        else:  # dot\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        attn_energies = attn_energies.t()\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "# Decoder with Attention and LSTM\n",
        "class LSTMAttnDecoder(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LSTMAttnDecoder, self).__init__()\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = embedding\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(embedding.embedding_dim, hidden_size, n_layers,\n",
        "                            dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        self.lstm.flatten_parameters()  # Fix for RNN warning\n",
        "        rnn_output, hidden = self.lstm(embedded, last_hidden)\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        return output, hidden\n"
      ],
      "metadata": {
        "id": "OyqmVZ2ncqRV"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initializes wandb api key = \"delete\"\n",
        "\n",
        "def init_wandb(project_name=\"seq2seqLSTM\", config=None):\n",
        "    if config is None:\n",
        "        config = {\n",
        "            \"model_name\": \"deep_lstm_seq2seq\",\n",
        "            \"attn_model\": \"dot\",\n",
        "            \"embedding\": \"glove.6B.300d\",\n",
        "            \"embedding_dim\": 300,  # match paper\n",
        "            \"freeze_embeddings\": False,\n",
        "            \"hidden_size\": 1000,  # deep hidden state\n",
        "            \"encoder_n_layers\": 4,\n",
        "            \"decoder_n_layers\": 4,\n",
        "            \"dropout\": 0.1,\n",
        "            \"batch_size\": 128,  # match paper\n",
        "            \"learning_rate\": 0.0001,  # fixed LR\n",
        "            \"decoder_learning_ratio\": 1.0,  # same as encoder\n",
        "            \"teacher_forcing_ratio\": 1.0,\n",
        "            \"clip\": 5.0,  # gradient norm clipping\n",
        "            \"n_iteration\": 10000,  # longer run\n",
        "            \"print_every\": 20,\n",
        "            \"save_every\": 1000\n",
        "        }\n",
        "\n",
        "    wandb.init(project=project_name, config=config)\n",
        "    return wandb.config\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2XuxlUX2fvpj"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0LUCUrC_dRWy",
        "outputId": "3e218401-9d9a-496b-f113-8cadf2bcc62a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-18 10:32:38--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2025-04-18 10:32:38--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2025-04-18 10:32:39--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.14MB/s    in 2m 39s  \n",
            "\n",
            "2025-04-18 10:35:19 (5.18 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "#from utils import Voc, load_glove_embeddings\n",
        "#from dataset import loadPrepareData, trimRareWords, split_dataset, get_dataloader\n",
        "#from models_seq2seq import LSTMEncoder, LSTMAttnDecoder\n",
        "#from wandb_config import init_wandb\n",
        "from datetime import datetime\n",
        "\n",
        "dev_config = init_wandb()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "datafile = \"/content/formatted_pairs.txt\"\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "corpus_name = \"custom\"\n",
        "voc, pairs = loadPrepareData(\"data\", corpus_name, datafile, save_dir)\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT=dev_config.MIN_COUNT if \"MIN_COUNT\" in dev_config else 3)\n",
        "\n",
        "train_pairs, val_pairs = split_dataset(pairs, test_size=0.1)\n",
        "\n",
        "train_loader = get_dataloader(train_pairs, voc, batch_size=dev_config.batch_size)\n",
        "val_loader = get_dataloader(val_pairs, voc, batch_size=dev_config.batch_size)\n",
        "\n",
        "embedding = load_glove_embeddings(\n",
        "    voc,\n",
        "    glove_path=\"/content/glove.6B.300d.txt\",\n",
        "    embedding_dim=dev_config.embedding_dim,\n",
        "    freeze=dev_config.freeze_embeddings\n",
        ")\n",
        "\n",
        "encoder = LSTMEncoder(dev_config.hidden_size, embedding, dev_config.encoder_n_layers, dev_config.dropout).to(device)\n",
        "decoder = LSTMAttnDecoder(dev_config.attn_model, embedding, dev_config.hidden_size, voc.num_words,\n",
        "                          dev_config.decoder_n_layers, dev_config.dropout).to(device)\n",
        "\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=dev_config.learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=dev_config.learning_rate * dev_config.decoder_learning_ratio)\n",
        "\n",
        "def save_checkpoint_tar(voc, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, iteration, loss, save_path=\"checkpoint.tar\"):\n",
        "    checkpoint = {\n",
        "        'iteration': iteration,\n",
        "        'encoder_state': encoder.state_dict(),\n",
        "        'decoder_state': decoder.state_dict(),\n",
        "        'embedding_state': embedding.state_dict(),\n",
        "        'encoder_optimizer_state': encoder_optimizer.state_dict(),\n",
        "        'decoder_optimizer_state': decoder_optimizer.state_dict(),\n",
        "        'voc_dict': voc.__dict__,\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, save_path)\n",
        "    with open(\"voc.pkl\", \"wb\") as f:\n",
        "        pickle.dump(voc, f)\n",
        "\n",
        "def log_artifacts_to_wandb(tar_path=\"checkpoint.tar\", voc_path=\"voc.pkl\", artifact_name=\"chatbot_model\"):\n",
        "    artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
        "    artifact.add_file(tar_path)\n",
        "    artifact.add_file(voc_path)\n",
        "    wandb.log_artifact(artifact)\n",
        "\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    return loss, nTotal.item()\n",
        "\n",
        "def train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "          encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, clip):\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    current_batch_size = input_variable.size(1)\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    encoder_outputs, (encoder_hidden, encoder_cell) = encoder(input_variable, lengths)\n",
        "    decoder_input = torch.LongTensor([[1 for _ in range(current_batch_size)]]).to(device)\n",
        "    decoder_hidden = (encoder_hidden[:decoder.n_layers], encoder_cell[:decoder.n_layers])\n",
        "\n",
        "    use_teacher_forcing = True if torch.rand(1).item() < dev_config.teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(current_batch_size)]]).to(device)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    loss.backward()\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), dev_config.clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), dev_config.clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals\n",
        "\n",
        "def evaluate_loss(val_loader, encoder, decoder, embedding):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    total_loss = 0\n",
        "    total_count = 0\n",
        "    with torch.no_grad():\n",
        "        for input_variable, lengths, target_variable, mask, max_target_len in val_loader:\n",
        "            input_variable = input_variable.to(device)\n",
        "            target_variable = target_variable.to(device)\n",
        "            mask = mask.to(device)\n",
        "            lengths = lengths.to(\"cpu\")\n",
        "\n",
        "            current_batch_size = input_variable.size(1)\n",
        "\n",
        "            encoder_outputs, (encoder_hidden, encoder_cell) = encoder(input_variable, lengths)\n",
        "            decoder_input = torch.LongTensor([[1 for _ in range(current_batch_size)]]).to(device)\n",
        "            decoder_hidden = (encoder_hidden[:decoder.n_layers], encoder_cell[:decoder.n_layers])\n",
        "\n",
        "            for t in range(max_target_len):\n",
        "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                decoder_input = target_variable[t].view(1, -1)\n",
        "                mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "                total_loss += mask_loss.item() * nTotal\n",
        "                total_count += nTotal\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    return total_loss / total_count\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "train_iter = iter(train_loader)\n",
        "for iteration in range(1, dev_config.n_iteration + 1):\n",
        "    try:\n",
        "        batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        batch = next(train_iter)\n",
        "\n",
        "    input_variable, lengths, target_variable, mask, max_target_len = batch\n",
        "    train_loss = train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "                       encoder, decoder, embedding, encoder_optimizer, decoder_optimizer,\n",
        "                       dev_config.clip)\n",
        "\n",
        "    perplexity = math.exp(train_loss)\n",
        "    wandb.log({\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_perplexity\": perplexity,\n",
        "        \"iteration\": iteration\n",
        "    })\n",
        "\n",
        "    if iteration % dev_config.print_every == 0:\n",
        "        print(\"Iteration: {}; Train Loss: {:.4f} | Perplexity: {:.4f}\".format(iteration, train_loss, perplexity))\n",
        "\n",
        "    if iteration % dev_config.save_every == 0:\n",
        "        val_loss = evaluate_loss(val_loader, encoder, decoder, embedding)\n",
        "        val_perplexity = math.exp(val_loss)\n",
        "        wandb.log({\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_perplexity\": val_perplexity\n",
        "        })\n",
        "        print(\"Validation Loss: {:.4f} | Perplexity: {:.4f}\".format(val_loss, val_perplexity))\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        save_checkpoint_tar(voc, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, iteration, val_loss, f\"checkpoint_{timestamp}.tar\")\n",
        "        log_artifacts_to_wandb(f\"checkpoint_{timestamp}.tar\", \"voc.pkl\", f\"chatbot_checkpoint_{iteration}\")\n",
        "\n",
        "save_checkpoint_tar(voc, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, iteration, train_loss, \"final_checkpoint.tar\")\n",
        "log_artifacts_to_wandb(\"final_checkpoint.tar\", \"voc.pkl\", \"final_chatbot_checkpoint\")\n",
        "print(\"Final checkpoint saved to W&B.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bF8YjkdfgEc9",
        "outputId": "a2bdc597-5731-44b0-b8f8-b910fb92cf55"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iteration</td><td>▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇██████</td></tr><tr><td>train_loss</td><td>▁                                       </td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iteration</td><td>221</td></tr><tr><td>train_loss</td><td>nan</td></tr><tr><td>train_perplexity</td><td>nan</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">serene-rain-4</strong> at: <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM/runs/flovw2wt' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM/runs/flovw2wt</a><br> View project at: <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250418_110235-flovw2wt/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250418_110512-a5do8d1w</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM/runs/a5do8d1w' target=\"_blank\">stoic-moon-5</a></strong> to <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM/runs/a5do8d1w' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM/runs/a5do8d1w</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 129656 sentence pairs\n",
            "Trimmed to 43827 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 16561\n",
            "keep_words 9888 / 16558 = 0.5972\n",
            "Trimmed from 43827 pairs to 34583, 0.7891 of total\n",
            "Split 34583 pairs into 31124 train and 3459 validation\n",
            "Loading GloVe embeddings...\n",
            "Found 9680/9891 words in GloVe.\n",
            "\n",
            "Starting training...\n",
            "Iteration: 20; Train Loss: 7.4572 | Perplexity: 1732.2342\n",
            "Iteration: 40; Train Loss: 6.2775 | Perplexity: 532.4702\n",
            "Iteration: 60; Train Loss: 6.1348 | Perplexity: 461.6549\n",
            "Iteration: 80; Train Loss: 6.0247 | Perplexity: 413.5223\n",
            "Iteration: 100; Train Loss: 6.0302 | Perplexity: 415.7794\n",
            "Iteration: 120; Train Loss: 6.1636 | Perplexity: 475.1329\n",
            "Iteration: 140; Train Loss: 6.1976 | Perplexity: 491.5755\n",
            "Iteration: 160; Train Loss: 6.0156 | Perplexity: 409.7908\n",
            "Iteration: 180; Train Loss: 5.8294 | Perplexity: 340.1674\n",
            "Iteration: 200; Train Loss: 6.0477 | Perplexity: 423.1266\n",
            "Iteration: 220; Train Loss: 6.0626 | Perplexity: 429.4791\n",
            "Iteration: 240; Train Loss: 5.8363 | Perplexity: 342.5002\n",
            "Iteration: 260; Train Loss: 5.8717 | Perplexity: 354.8494\n",
            "Iteration: 280; Train Loss: 5.7621 | Perplexity: 318.0271\n",
            "Iteration: 300; Train Loss: 5.9045 | Perplexity: 366.6864\n",
            "Iteration: 320; Train Loss: 5.7203 | Perplexity: 304.9851\n",
            "Iteration: 340; Train Loss: 5.9387 | Perplexity: 379.4406\n",
            "Iteration: 360; Train Loss: 5.8414 | Perplexity: 344.2659\n",
            "Iteration: 380; Train Loss: 5.7689 | Perplexity: 320.1766\n",
            "Iteration: 400; Train Loss: 5.6128 | Perplexity: 273.9021\n",
            "Iteration: 420; Train Loss: 5.7054 | Perplexity: 300.4953\n",
            "Iteration: 440; Train Loss: 5.6905 | Perplexity: 296.0356\n",
            "Iteration: 460; Train Loss: 5.6166 | Perplexity: 274.9426\n",
            "Iteration: 480; Train Loss: 5.7072 | Perplexity: 301.0262\n",
            "Iteration: 500; Train Loss: 5.5770 | Perplexity: 264.2905\n",
            "Iteration: 520; Train Loss: 5.5985 | Perplexity: 270.0228\n",
            "Iteration: 540; Train Loss: 5.8108 | Perplexity: 333.8761\n",
            "Iteration: 560; Train Loss: 5.5349 | Perplexity: 253.3906\n",
            "Iteration: 580; Train Loss: 5.3189 | Perplexity: 204.1493\n",
            "Iteration: 600; Train Loss: 5.2656 | Perplexity: 193.5567\n",
            "Iteration: 620; Train Loss: 5.3863 | Perplexity: 218.3875\n",
            "Iteration: 640; Train Loss: 5.2886 | Perplexity: 198.0691\n",
            "Iteration: 660; Train Loss: 5.3481 | Perplexity: 210.2049\n",
            "Iteration: 680; Train Loss: 5.4176 | Perplexity: 225.3367\n",
            "Iteration: 700; Train Loss: 5.3647 | Perplexity: 213.7375\n",
            "Iteration: 720; Train Loss: 5.2788 | Perplexity: 196.1375\n",
            "Iteration: 740; Train Loss: 5.0777 | Perplexity: 160.4007\n",
            "Iteration: 760; Train Loss: 5.1309 | Perplexity: 169.1746\n",
            "Iteration: 780; Train Loss: 5.1738 | Perplexity: 176.5914\n",
            "Iteration: 800; Train Loss: 5.0893 | Perplexity: 162.2779\n",
            "Iteration: 820; Train Loss: 4.9473 | Perplexity: 140.7986\n",
            "Iteration: 840; Train Loss: 5.3179 | Perplexity: 203.9556\n",
            "Iteration: 860; Train Loss: 5.1366 | Perplexity: 170.1410\n",
            "Iteration: 880; Train Loss: 5.1016 | Perplexity: 164.2788\n",
            "Iteration: 900; Train Loss: 4.8398 | Perplexity: 126.4399\n",
            "Iteration: 920; Train Loss: 5.0185 | Perplexity: 151.1918\n",
            "Iteration: 940; Train Loss: 5.0892 | Perplexity: 162.2634\n",
            "Iteration: 960; Train Loss: 5.0597 | Perplexity: 157.5477\n",
            "Iteration: 980; Train Loss: 5.0383 | Perplexity: 154.2140\n",
            "Iteration: 1000; Train Loss: 5.0118 | Perplexity: 150.1678\n",
            "Validation Loss: 5.0036 | Perplexity: 148.9520\n",
            "Iteration: 1020; Train Loss: 4.9590 | Perplexity: 142.4487\n",
            "Iteration: 1040; Train Loss: 4.9789 | Perplexity: 145.3217\n",
            "Iteration: 1060; Train Loss: 4.6836 | Perplexity: 108.1598\n",
            "Iteration: 1080; Train Loss: 4.8664 | Perplexity: 129.8565\n",
            "Iteration: 1100; Train Loss: 4.8793 | Perplexity: 131.5321\n",
            "Iteration: 1120; Train Loss: 4.8392 | Perplexity: 126.3638\n",
            "Iteration: 1140; Train Loss: 4.7124 | Perplexity: 111.3192\n",
            "Iteration: 1160; Train Loss: 4.8868 | Perplexity: 132.5354\n",
            "Iteration: 1180; Train Loss: 4.7169 | Perplexity: 111.8202\n",
            "Iteration: 1200; Train Loss: 4.6789 | Perplexity: 107.6469\n",
            "Iteration: 1220; Train Loss: 4.5601 | Perplexity: 95.5899\n",
            "Iteration: 1240; Train Loss: 4.6837 | Perplexity: 108.1688\n",
            "Iteration: 1260; Train Loss: 4.6108 | Perplexity: 100.5675\n",
            "Iteration: 1280; Train Loss: 4.6334 | Perplexity: 102.8587\n",
            "Iteration: 1300; Train Loss: 4.5457 | Perplexity: 94.2283\n",
            "Iteration: 1320; Train Loss: 4.6209 | Perplexity: 101.5899\n",
            "Iteration: 1340; Train Loss: 4.4147 | Perplexity: 82.6544\n",
            "Iteration: 1360; Train Loss: 4.6880 | Perplexity: 108.6392\n",
            "Iteration: 1380; Train Loss: 4.5905 | Perplexity: 98.5428\n",
            "Iteration: 1400; Train Loss: 4.6012 | Perplexity: 99.6012\n",
            "Iteration: 1420; Train Loss: 4.3261 | Perplexity: 75.6491\n",
            "Iteration: 1440; Train Loss: 4.4461 | Perplexity: 85.2960\n",
            "Iteration: 1460; Train Loss: 4.3328 | Perplexity: 76.1561\n",
            "Iteration: 1480; Train Loss: 4.4544 | Perplexity: 86.0056\n",
            "Iteration: 1500; Train Loss: 4.2417 | Perplexity: 69.5231\n",
            "Iteration: 1520; Train Loss: 4.2999 | Perplexity: 73.6889\n",
            "Iteration: 1540; Train Loss: 4.3180 | Perplexity: 75.0355\n",
            "Iteration: 1560; Train Loss: 4.2525 | Perplexity: 70.2817\n",
            "Iteration: 1580; Train Loss: 4.3324 | Perplexity: 76.1236\n",
            "Iteration: 1600; Train Loss: 4.3121 | Perplexity: 74.5988\n",
            "Iteration: 1620; Train Loss: 4.5080 | Perplexity: 90.7444\n",
            "Iteration: 1640; Train Loss: 4.2862 | Perplexity: 72.6866\n",
            "Iteration: 1660; Train Loss: 4.2845 | Perplexity: 72.5629\n",
            "Iteration: 1680; Train Loss: 4.0992 | Perplexity: 60.2927\n",
            "Iteration: 1700; Train Loss: 4.5006 | Perplexity: 90.0723\n",
            "Iteration: 1720; Train Loss: 4.2777 | Perplexity: 72.0754\n",
            "Iteration: 1740; Train Loss: 4.3179 | Perplexity: 75.0294\n",
            "Iteration: 1760; Train Loss: 3.9769 | Perplexity: 53.3493\n",
            "Iteration: 1780; Train Loss: 4.2767 | Perplexity: 72.0044\n",
            "Iteration: 1800; Train Loss: 4.2324 | Perplexity: 68.8812\n",
            "Iteration: 1820; Train Loss: 4.1498 | Perplexity: 63.4231\n",
            "Iteration: 1840; Train Loss: 4.0288 | Perplexity: 56.1961\n",
            "Iteration: 1860; Train Loss: 4.2861 | Perplexity: 72.6805\n",
            "Iteration: 1880; Train Loss: 4.1485 | Perplexity: 63.3374\n",
            "Iteration: 1900; Train Loss: 4.3174 | Perplexity: 74.9910\n",
            "Iteration: 1920; Train Loss: 4.2646 | Perplexity: 71.1336\n",
            "Iteration: 1940; Train Loss: 4.1020 | Perplexity: 60.4625\n",
            "Iteration: 1960; Train Loss: 3.8582 | Perplexity: 47.3777\n",
            "Iteration: 1980; Train Loss: 3.8820 | Perplexity: 48.5212\n",
            "Iteration: 2000; Train Loss: 3.9812 | Perplexity: 53.5804\n",
            "Validation Loss: 4.3443 | Perplexity: 77.0371\n",
            "Iteration: 2020; Train Loss: 4.1077 | Perplexity: 60.8081\n",
            "Iteration: 2040; Train Loss: 4.0042 | Perplexity: 54.8267\n",
            "Iteration: 2060; Train Loss: 3.9953 | Perplexity: 54.3440\n",
            "Iteration: 2080; Train Loss: 3.9777 | Perplexity: 53.3949\n",
            "Iteration: 2100; Train Loss: 3.8942 | Perplexity: 49.1143\n",
            "Iteration: 2120; Train Loss: 4.0840 | Perplexity: 59.3826\n",
            "Iteration: 2140; Train Loss: 3.9499 | Perplexity: 51.9282\n",
            "Iteration: 2160; Train Loss: 3.9549 | Perplexity: 52.1927\n",
            "Iteration: 2180; Train Loss: 3.9509 | Perplexity: 51.9837\n",
            "Iteration: 2200; Train Loss: 3.8850 | Perplexity: 48.6682\n",
            "Iteration: 2220; Train Loss: 3.6021 | Perplexity: 36.6736\n",
            "Iteration: 2240; Train Loss: 3.7365 | Perplexity: 41.9524\n",
            "Iteration: 2260; Train Loss: 3.8138 | Perplexity: 45.3211\n",
            "Iteration: 2280; Train Loss: 3.8955 | Perplexity: 49.1817\n",
            "Iteration: 2300; Train Loss: 3.8946 | Perplexity: 49.1364\n",
            "Iteration: 2320; Train Loss: 4.1228 | Perplexity: 61.7288\n",
            "Iteration: 2340; Train Loss: 3.9865 | Perplexity: 53.8645\n",
            "Iteration: 2360; Train Loss: 3.9005 | Perplexity: 49.4248\n",
            "Iteration: 2380; Train Loss: 3.7961 | Perplexity: 44.5253\n",
            "Iteration: 2400; Train Loss: 3.6385 | Perplexity: 38.0342\n",
            "Iteration: 2420; Train Loss: 3.8430 | Perplexity: 46.6642\n",
            "Iteration: 2440; Train Loss: 3.5647 | Perplexity: 35.3288\n",
            "Iteration: 2460; Train Loss: 3.7294 | Perplexity: 41.6541\n",
            "Iteration: 2480; Train Loss: 3.6302 | Perplexity: 37.7222\n",
            "Iteration: 2500; Train Loss: 3.7682 | Perplexity: 43.3025\n",
            "Iteration: 2520; Train Loss: 3.5930 | Perplexity: 36.3431\n",
            "Iteration: 2540; Train Loss: 3.8824 | Perplexity: 48.5392\n",
            "Iteration: 2560; Train Loss: 3.5965 | Perplexity: 36.4708\n",
            "Iteration: 2580; Train Loss: 3.7992 | Perplexity: 44.6637\n",
            "Iteration: 2600; Train Loss: 3.5453 | Perplexity: 34.6486\n",
            "Iteration: 2620; Train Loss: 3.7371 | Perplexity: 41.9769\n",
            "Iteration: 2640; Train Loss: 3.6344 | Perplexity: 37.8772\n",
            "Iteration: 2660; Train Loss: 3.6415 | Perplexity: 38.1476\n",
            "Iteration: 2680; Train Loss: 3.6258 | Perplexity: 37.5566\n",
            "Iteration: 2700; Train Loss: 3.4788 | Perplexity: 32.4192\n",
            "Iteration: 2720; Train Loss: 3.4994 | Perplexity: 33.0962\n",
            "Iteration: 2740; Train Loss: 3.4778 | Perplexity: 32.3878\n",
            "Iteration: 2760; Train Loss: 3.4575 | Perplexity: 31.7377\n",
            "Iteration: 2780; Train Loss: 3.6728 | Perplexity: 39.3601\n",
            "Iteration: 2800; Train Loss: 3.6396 | Perplexity: 38.0781\n",
            "Iteration: 2820; Train Loss: 3.6972 | Perplexity: 40.3343\n",
            "Iteration: 2840; Train Loss: 3.5633 | Perplexity: 35.2786\n",
            "Iteration: 2860; Train Loss: 3.4594 | Perplexity: 31.7979\n",
            "Iteration: 2880; Train Loss: 3.4117 | Perplexity: 30.3181\n",
            "Iteration: 2900; Train Loss: 3.7020 | Perplexity: 40.5293\n",
            "Iteration: 2920; Train Loss: 3.4951 | Perplexity: 32.9538\n",
            "Iteration: 2940; Train Loss: 3.4712 | Perplexity: 32.1752\n",
            "Iteration: 2960; Train Loss: 3.4479 | Perplexity: 31.4348\n",
            "Iteration: 2980; Train Loss: 3.2166 | Perplexity: 24.9438\n",
            "Iteration: 3000; Train Loss: 3.3842 | Perplexity: 29.4941\n",
            "Validation Loss: 3.9312 | Perplexity: 50.9680\n",
            "Iteration: 3020; Train Loss: 3.5325 | Perplexity: 34.2081\n",
            "Iteration: 3040; Train Loss: 3.4415 | Perplexity: 31.2326\n",
            "Iteration: 3060; Train Loss: 3.3073 | Perplexity: 27.3118\n",
            "Iteration: 3080; Train Loss: 3.4567 | Perplexity: 31.7122\n",
            "Iteration: 3100; Train Loss: 3.5493 | Perplexity: 34.7898\n",
            "Iteration: 3120; Train Loss: 3.4269 | Perplexity: 30.7806\n",
            "Iteration: 3140; Train Loss: 3.3888 | Perplexity: 29.6306\n",
            "Iteration: 3160; Train Loss: 3.2763 | Perplexity: 26.4767\n",
            "Iteration: 3180; Train Loss: 3.3789 | Perplexity: 29.3398\n",
            "Iteration: 3200; Train Loss: 3.3362 | Perplexity: 28.1110\n",
            "Iteration: 3220; Train Loss: 3.3592 | Perplexity: 28.7663\n",
            "Iteration: 3240; Train Loss: 3.2388 | Perplexity: 25.5041\n",
            "Iteration: 3260; Train Loss: 3.0924 | Perplexity: 22.0307\n",
            "Iteration: 3280; Train Loss: 3.1232 | Perplexity: 22.7190\n",
            "Iteration: 3300; Train Loss: 3.1835 | Perplexity: 24.1311\n",
            "Iteration: 3320; Train Loss: 3.3488 | Perplexity: 28.4681\n",
            "Iteration: 3340; Train Loss: 3.2680 | Perplexity: 26.2586\n",
            "Iteration: 3360; Train Loss: 3.3314 | Perplexity: 27.9774\n",
            "Iteration: 3380; Train Loss: 3.5522 | Perplexity: 34.8893\n",
            "Iteration: 3400; Train Loss: 3.4330 | Perplexity: 30.9695\n",
            "Iteration: 3420; Train Loss: 3.1759 | Perplexity: 23.9494\n",
            "Iteration: 3440; Train Loss: 3.2260 | Perplexity: 25.1796\n",
            "Iteration: 3460; Train Loss: 3.1075 | Perplexity: 22.3649\n",
            "Iteration: 3480; Train Loss: 3.1525 | Perplexity: 23.3935\n",
            "Iteration: 3500; Train Loss: 3.1705 | Perplexity: 23.8196\n",
            "Iteration: 3520; Train Loss: 3.2288 | Perplexity: 25.2491\n",
            "Iteration: 3540; Train Loss: 3.1540 | Perplexity: 23.4291\n",
            "Iteration: 3560; Train Loss: 3.1594 | Perplexity: 23.5574\n",
            "Iteration: 3580; Train Loss: 3.0566 | Perplexity: 21.2541\n",
            "Iteration: 3600; Train Loss: 3.2073 | Perplexity: 24.7124\n",
            "Iteration: 3620; Train Loss: 3.1947 | Perplexity: 24.4026\n",
            "Iteration: 3640; Train Loss: 3.0009 | Perplexity: 20.1030\n",
            "Iteration: 3660; Train Loss: 3.4953 | Perplexity: 32.9607\n",
            "Iteration: 3680; Train Loss: 3.0288 | Perplexity: 20.6720\n",
            "Iteration: 3700; Train Loss: 3.0551 | Perplexity: 21.2243\n",
            "Iteration: 3720; Train Loss: 3.0998 | Perplexity: 22.1943\n",
            "Iteration: 3740; Train Loss: 3.0505 | Perplexity: 21.1256\n",
            "Iteration: 3760; Train Loss: 3.1434 | Perplexity: 23.1818\n",
            "Iteration: 3780; Train Loss: 3.0890 | Perplexity: 21.9548\n",
            "Iteration: 3800; Train Loss: 2.8614 | Perplexity: 17.4856\n",
            "Iteration: 3820; Train Loss: 3.0600 | Perplexity: 21.3275\n",
            "Iteration: 3840; Train Loss: 2.9021 | Perplexity: 18.2127\n",
            "Iteration: 3860; Train Loss: 3.1856 | Perplexity: 24.1813\n",
            "Iteration: 3880; Train Loss: 3.1988 | Perplexity: 24.5022\n",
            "Iteration: 3900; Train Loss: 3.0076 | Perplexity: 20.2394\n",
            "Iteration: 3920; Train Loss: 2.7675 | Perplexity: 15.9191\n",
            "Iteration: 3940; Train Loss: 2.7597 | Perplexity: 15.7943\n",
            "Iteration: 3960; Train Loss: 2.8196 | Perplexity: 16.7704\n",
            "Iteration: 3980; Train Loss: 2.9630 | Perplexity: 19.3552\n",
            "Iteration: 4000; Train Loss: 2.8654 | Perplexity: 17.5564\n",
            "Validation Loss: 3.5572 | Perplexity: 35.0658\n",
            "Iteration: 4020; Train Loss: 2.8319 | Perplexity: 16.9781\n",
            "Iteration: 4040; Train Loss: 3.0104 | Perplexity: 20.2951\n",
            "Iteration: 4060; Train Loss: 3.0042 | Perplexity: 20.1703\n",
            "Iteration: 4080; Train Loss: 2.9298 | Perplexity: 18.7240\n",
            "Iteration: 4100; Train Loss: 2.8031 | Perplexity: 16.4960\n",
            "Iteration: 4120; Train Loss: 2.9694 | Perplexity: 19.4794\n",
            "Iteration: 4140; Train Loss: 2.7867 | Perplexity: 16.2266\n",
            "Iteration: 4160; Train Loss: 2.7888 | Perplexity: 16.2609\n",
            "Iteration: 4180; Train Loss: 2.9490 | Perplexity: 19.0875\n",
            "Iteration: 4200; Train Loss: 2.7790 | Perplexity: 16.1030\n",
            "Iteration: 4220; Train Loss: 2.6584 | Perplexity: 14.2733\n",
            "Iteration: 4240; Train Loss: 2.7687 | Perplexity: 15.9382\n",
            "Iteration: 4260; Train Loss: 2.7655 | Perplexity: 15.8877\n",
            "Iteration: 4280; Train Loss: 2.9327 | Perplexity: 18.7782\n",
            "Iteration: 4300; Train Loss: 2.6659 | Perplexity: 14.3806\n",
            "Iteration: 4320; Train Loss: 2.8595 | Perplexity: 17.4531\n",
            "Iteration: 4340; Train Loss: 2.7392 | Perplexity: 15.4738\n",
            "Iteration: 4360; Train Loss: 2.7524 | Perplexity: 15.6809\n",
            "Iteration: 4380; Train Loss: 2.8797 | Perplexity: 17.8090\n",
            "Iteration: 4400; Train Loss: 2.7729 | Perplexity: 16.0050\n",
            "Iteration: 4420; Train Loss: 2.7189 | Perplexity: 15.1632\n",
            "Iteration: 4440; Train Loss: 2.7221 | Perplexity: 15.2115\n",
            "Iteration: 4460; Train Loss: 2.5717 | Perplexity: 13.0881\n",
            "Iteration: 4480; Train Loss: 2.6276 | Perplexity: 13.8399\n",
            "Iteration: 4500; Train Loss: 2.5644 | Perplexity: 12.9929\n",
            "Iteration: 4520; Train Loss: 2.6129 | Perplexity: 13.6383\n",
            "Iteration: 4540; Train Loss: 2.5179 | Perplexity: 12.4022\n",
            "Iteration: 4560; Train Loss: 2.6974 | Perplexity: 14.8410\n",
            "Iteration: 4580; Train Loss: 2.5850 | Perplexity: 13.2634\n",
            "Iteration: 4600; Train Loss: 2.7610 | Perplexity: 15.8152\n",
            "Iteration: 4620; Train Loss: 2.7397 | Perplexity: 15.4822\n",
            "Iteration: 4640; Train Loss: 2.5120 | Perplexity: 12.3298\n",
            "Iteration: 4660; Train Loss: 2.5198 | Perplexity: 12.4266\n",
            "Iteration: 4680; Train Loss: 2.6610 | Perplexity: 14.3106\n",
            "Iteration: 4700; Train Loss: 2.4405 | Perplexity: 11.4791\n",
            "Iteration: 4720; Train Loss: 2.5711 | Perplexity: 13.0799\n",
            "Iteration: 4740; Train Loss: 2.5426 | Perplexity: 12.7124\n",
            "Iteration: 4760; Train Loss: 2.4438 | Perplexity: 11.5168\n",
            "Iteration: 4780; Train Loss: 2.4855 | Perplexity: 12.0075\n",
            "Iteration: 4800; Train Loss: 2.5864 | Perplexity: 13.2816\n",
            "Iteration: 4820; Train Loss: 2.6180 | Perplexity: 13.7087\n",
            "Iteration: 4840; Train Loss: 2.2967 | Perplexity: 9.9412\n",
            "Iteration: 4860; Train Loss: 2.4383 | Perplexity: 11.4540\n",
            "Iteration: 4880; Train Loss: 2.3852 | Perplexity: 10.8615\n",
            "Iteration: 4900; Train Loss: 2.2834 | Perplexity: 9.8102\n",
            "Iteration: 4920; Train Loss: 2.1835 | Perplexity: 8.8769\n",
            "Iteration: 4940; Train Loss: 2.4081 | Perplexity: 11.1127\n",
            "Iteration: 4960; Train Loss: 2.5921 | Perplexity: 13.3583\n",
            "Iteration: 4980; Train Loss: 2.5635 | Perplexity: 12.9814\n",
            "Iteration: 5000; Train Loss: 2.4743 | Perplexity: 11.8736\n",
            "Validation Loss: 3.1869 | Perplexity: 24.2126\n",
            "Iteration: 5020; Train Loss: 2.3905 | Perplexity: 10.9192\n",
            "Iteration: 5040; Train Loss: 2.4397 | Perplexity: 11.4694\n",
            "Iteration: 5060; Train Loss: 2.5358 | Perplexity: 12.6263\n",
            "Iteration: 5080; Train Loss: 2.2966 | Perplexity: 9.9404\n",
            "Iteration: 5100; Train Loss: 2.4438 | Perplexity: 11.5164\n",
            "Iteration: 5120; Train Loss: 2.4412 | Perplexity: 11.4869\n",
            "Iteration: 5140; Train Loss: 2.2930 | Perplexity: 9.9045\n",
            "Iteration: 5160; Train Loss: 2.2710 | Perplexity: 9.6888\n",
            "Iteration: 5180; Train Loss: 2.2747 | Perplexity: 9.7253\n",
            "Iteration: 5200; Train Loss: 2.2057 | Perplexity: 9.0764\n",
            "Iteration: 5220; Train Loss: 2.1834 | Perplexity: 8.8768\n",
            "Iteration: 5240; Train Loss: 2.1336 | Perplexity: 8.4453\n",
            "Iteration: 5260; Train Loss: 2.2565 | Perplexity: 9.5494\n",
            "Iteration: 5280; Train Loss: 2.1760 | Perplexity: 8.8111\n",
            "Iteration: 5300; Train Loss: 2.3191 | Perplexity: 10.1662\n",
            "Iteration: 5320; Train Loss: 2.2381 | Perplexity: 9.3755\n",
            "Iteration: 5340; Train Loss: 2.2886 | Perplexity: 9.8612\n",
            "Iteration: 5360; Train Loss: 2.2951 | Perplexity: 9.9250\n",
            "Iteration: 5380; Train Loss: 2.1904 | Perplexity: 8.9385\n",
            "Iteration: 5400; Train Loss: 2.1505 | Perplexity: 8.5890\n",
            "Iteration: 5420; Train Loss: 2.1436 | Perplexity: 8.5300\n",
            "Iteration: 5440; Train Loss: 2.3007 | Perplexity: 9.9814\n",
            "Iteration: 5460; Train Loss: 2.1102 | Perplexity: 8.2498\n",
            "Iteration: 5480; Train Loss: 1.9326 | Perplexity: 6.9075\n",
            "Iteration: 5500; Train Loss: 2.1315 | Perplexity: 8.4271\n",
            "Iteration: 5520; Train Loss: 2.1524 | Perplexity: 8.6051\n",
            "Iteration: 5540; Train Loss: 2.3313 | Perplexity: 10.2912\n",
            "Iteration: 5560; Train Loss: 2.3520 | Perplexity: 10.5070\n",
            "Iteration: 5580; Train Loss: 2.1550 | Perplexity: 8.6279\n",
            "Iteration: 5600; Train Loss: 2.3292 | Perplexity: 10.2699\n",
            "Iteration: 5620; Train Loss: 1.9773 | Perplexity: 7.2231\n",
            "Iteration: 5640; Train Loss: 2.1130 | Perplexity: 8.2733\n",
            "Iteration: 5660; Train Loss: 2.0044 | Perplexity: 7.4218\n",
            "Iteration: 5680; Train Loss: 2.0776 | Perplexity: 7.9851\n",
            "Iteration: 5700; Train Loss: 2.2271 | Perplexity: 9.2727\n",
            "Iteration: 5720; Train Loss: 1.9356 | Perplexity: 6.9285\n",
            "Iteration: 5740; Train Loss: 2.2098 | Perplexity: 9.1140\n",
            "Iteration: 5760; Train Loss: 1.8906 | Perplexity: 6.6236\n",
            "Iteration: 5780; Train Loss: 1.9835 | Perplexity: 7.2681\n",
            "Iteration: 5800; Train Loss: 2.0538 | Perplexity: 7.7977\n",
            "Iteration: 5820; Train Loss: 2.2184 | Perplexity: 9.1923\n",
            "Iteration: 5840; Train Loss: 2.0816 | Perplexity: 8.0175\n",
            "Iteration: 5860; Train Loss: 1.9074 | Perplexity: 6.7359\n",
            "Iteration: 5880; Train Loss: 1.9705 | Perplexity: 7.1740\n",
            "Iteration: 5900; Train Loss: 2.1498 | Perplexity: 8.5828\n",
            "Iteration: 5920; Train Loss: 2.0197 | Perplexity: 7.5358\n",
            "Iteration: 5940; Train Loss: 1.8939 | Perplexity: 6.6449\n",
            "Iteration: 5960; Train Loss: 1.9203 | Perplexity: 6.8232\n",
            "Iteration: 5980; Train Loss: 2.0789 | Perplexity: 7.9958\n",
            "Iteration: 6000; Train Loss: 1.8838 | Perplexity: 6.5782\n",
            "Validation Loss: 2.8172 | Perplexity: 16.7295\n",
            "Iteration: 6020; Train Loss: 2.0936 | Perplexity: 8.1137\n",
            "Iteration: 6040; Train Loss: 1.9358 | Perplexity: 6.9293\n",
            "Iteration: 6060; Train Loss: 1.9371 | Perplexity: 6.9387\n",
            "Iteration: 6080; Train Loss: 1.9101 | Perplexity: 6.7540\n",
            "Iteration: 6100; Train Loss: 1.9887 | Perplexity: 7.3062\n",
            "Iteration: 6120; Train Loss: 1.7358 | Perplexity: 5.6733\n",
            "Iteration: 6140; Train Loss: 1.7479 | Perplexity: 5.7425\n",
            "Iteration: 6160; Train Loss: 1.9007 | Perplexity: 6.6906\n",
            "Iteration: 6180; Train Loss: 1.8051 | Perplexity: 6.0807\n",
            "Iteration: 6200; Train Loss: 1.9565 | Perplexity: 7.0746\n",
            "Iteration: 6220; Train Loss: 1.7197 | Perplexity: 5.5828\n",
            "Iteration: 6240; Train Loss: 1.9105 | Perplexity: 6.7563\n",
            "Iteration: 6260; Train Loss: 1.6584 | Perplexity: 5.2510\n",
            "Iteration: 6280; Train Loss: 1.8388 | Perplexity: 6.2889\n",
            "Iteration: 6300; Train Loss: 1.7341 | Perplexity: 5.6641\n",
            "Iteration: 6320; Train Loss: 1.7645 | Perplexity: 5.8388\n",
            "Iteration: 6340; Train Loss: 1.8559 | Perplexity: 6.3976\n",
            "Iteration: 6360; Train Loss: 1.7738 | Perplexity: 5.8930\n",
            "Iteration: 6380; Train Loss: 1.6010 | Perplexity: 4.9579\n",
            "Iteration: 6400; Train Loss: 1.7186 | Perplexity: 5.5767\n",
            "Iteration: 6420; Train Loss: 1.6461 | Perplexity: 5.1868\n",
            "Iteration: 6440; Train Loss: 1.7965 | Perplexity: 6.0286\n",
            "Iteration: 6460; Train Loss: 1.7319 | Perplexity: 5.6513\n",
            "Iteration: 6480; Train Loss: 1.7631 | Perplexity: 5.8307\n",
            "Iteration: 6500; Train Loss: 1.6886 | Perplexity: 5.4119\n",
            "Iteration: 6520; Train Loss: 1.5788 | Perplexity: 4.8492\n",
            "Iteration: 6540; Train Loss: 1.8604 | Perplexity: 6.4262\n",
            "Iteration: 6560; Train Loss: 1.6714 | Perplexity: 5.3197\n",
            "Iteration: 6580; Train Loss: 1.5986 | Perplexity: 4.9459\n",
            "Iteration: 6600; Train Loss: 1.5384 | Perplexity: 4.6573\n",
            "Iteration: 6620; Train Loss: 1.5164 | Perplexity: 4.5558\n",
            "Iteration: 6640; Train Loss: 1.4006 | Perplexity: 4.0574\n",
            "Iteration: 6660; Train Loss: 1.5093 | Perplexity: 4.5236\n",
            "Iteration: 6680; Train Loss: 1.5663 | Perplexity: 4.7887\n",
            "Iteration: 6700; Train Loss: 1.5361 | Perplexity: 4.6465\n",
            "Iteration: 6720; Train Loss: 1.6087 | Perplexity: 4.9965\n",
            "Iteration: 6740; Train Loss: 1.6030 | Perplexity: 4.9681\n",
            "Iteration: 6760; Train Loss: 1.5996 | Perplexity: 4.9512\n",
            "Iteration: 6780; Train Loss: 1.6364 | Perplexity: 5.1368\n",
            "Iteration: 6800; Train Loss: 1.5108 | Perplexity: 4.5302\n",
            "Iteration: 6820; Train Loss: 1.5017 | Perplexity: 4.4891\n",
            "Iteration: 6840; Train Loss: 1.4768 | Perplexity: 4.3790\n",
            "Iteration: 6860; Train Loss: 1.5492 | Perplexity: 4.7079\n",
            "Iteration: 6880; Train Loss: 1.5168 | Perplexity: 4.5574\n",
            "Iteration: 6900; Train Loss: 1.4074 | Perplexity: 4.0854\n",
            "Iteration: 6920; Train Loss: 1.5698 | Perplexity: 4.8058\n",
            "Iteration: 6940; Train Loss: 1.4144 | Perplexity: 4.1139\n",
            "Iteration: 6960; Train Loss: 1.5220 | Perplexity: 4.5816\n",
            "Iteration: 6980; Train Loss: 1.4591 | Perplexity: 4.3020\n",
            "Iteration: 7000; Train Loss: 1.4824 | Perplexity: 4.4035\n",
            "Validation Loss: 2.4416 | Perplexity: 11.4919\n",
            "Iteration: 7020; Train Loss: 1.3257 | Perplexity: 3.7650\n",
            "Iteration: 7040; Train Loss: 1.4644 | Perplexity: 4.3251\n",
            "Iteration: 7060; Train Loss: 1.4476 | Perplexity: 4.2528\n",
            "Iteration: 7080; Train Loss: 1.2535 | Perplexity: 3.5025\n",
            "Iteration: 7100; Train Loss: 1.3622 | Perplexity: 3.9048\n",
            "Iteration: 7120; Train Loss: 1.4338 | Perplexity: 4.1947\n",
            "Iteration: 7140; Train Loss: 1.3266 | Perplexity: 3.7682\n",
            "Iteration: 7160; Train Loss: 1.3071 | Perplexity: 3.6956\n",
            "Iteration: 7180; Train Loss: 1.3027 | Perplexity: 3.6792\n",
            "Iteration: 7200; Train Loss: 1.4196 | Perplexity: 4.1353\n",
            "Iteration: 7220; Train Loss: 1.3844 | Perplexity: 3.9924\n",
            "Iteration: 7240; Train Loss: 1.3364 | Perplexity: 3.8054\n",
            "Iteration: 7260; Train Loss: 1.4358 | Perplexity: 4.2030\n",
            "Iteration: 7280; Train Loss: 1.3688 | Perplexity: 3.9306\n",
            "Iteration: 7300; Train Loss: 1.5933 | Perplexity: 4.9201\n",
            "Iteration: 7320; Train Loss: 1.3014 | Perplexity: 3.6744\n",
            "Iteration: 7340; Train Loss: 1.2218 | Perplexity: 3.3933\n",
            "Iteration: 7360; Train Loss: 1.1800 | Perplexity: 3.2543\n",
            "Iteration: 7380; Train Loss: 1.2672 | Perplexity: 3.5508\n",
            "Iteration: 7400; Train Loss: 1.2584 | Perplexity: 3.5199\n",
            "Iteration: 7420; Train Loss: 1.2716 | Perplexity: 3.5665\n",
            "Iteration: 7440; Train Loss: 1.3535 | Perplexity: 3.8708\n",
            "Iteration: 7460; Train Loss: 1.2318 | Perplexity: 3.4274\n",
            "Iteration: 7480; Train Loss: 1.3844 | Perplexity: 3.9926\n",
            "Iteration: 7500; Train Loss: 1.1501 | Perplexity: 3.1585\n",
            "Iteration: 7520; Train Loss: 1.2214 | Perplexity: 3.3919\n",
            "Iteration: 7540; Train Loss: 1.2110 | Perplexity: 3.3568\n",
            "Iteration: 7560; Train Loss: 1.2633 | Perplexity: 3.5372\n",
            "Iteration: 7580; Train Loss: 1.1085 | Perplexity: 3.0297\n",
            "Iteration: 7600; Train Loss: 1.1099 | Perplexity: 3.0339\n",
            "Iteration: 7620; Train Loss: 1.1150 | Perplexity: 3.0496\n",
            "Iteration: 7640; Train Loss: 1.1911 | Perplexity: 3.2908\n",
            "Iteration: 7660; Train Loss: 1.1542 | Perplexity: 3.1715\n",
            "Iteration: 7680; Train Loss: 1.2395 | Perplexity: 3.4537\n",
            "Iteration: 7700; Train Loss: 1.1939 | Perplexity: 3.3001\n",
            "Iteration: 7720; Train Loss: 1.1091 | Perplexity: 3.0317\n",
            "Iteration: 7740; Train Loss: 1.0307 | Perplexity: 2.8029\n",
            "Iteration: 7760; Train Loss: 1.2455 | Perplexity: 3.4747\n",
            "Iteration: 7780; Train Loss: 1.2644 | Perplexity: 3.5408\n",
            "Iteration: 7800; Train Loss: 1.2179 | Perplexity: 3.3801\n",
            "Iteration: 7820; Train Loss: 1.0696 | Perplexity: 2.9143\n",
            "Iteration: 7840; Train Loss: 1.0275 | Perplexity: 2.7940\n",
            "Iteration: 7860; Train Loss: 1.0635 | Perplexity: 2.8965\n",
            "Iteration: 7880; Train Loss: 1.1361 | Perplexity: 3.1146\n",
            "Iteration: 7900; Train Loss: 1.0877 | Perplexity: 2.9674\n",
            "Iteration: 7920; Train Loss: 1.0666 | Perplexity: 2.9054\n",
            "Iteration: 7940; Train Loss: 1.1895 | Perplexity: 3.2856\n",
            "Iteration: 7960; Train Loss: 1.0931 | Perplexity: 2.9834\n",
            "Iteration: 7980; Train Loss: 1.0524 | Perplexity: 2.8644\n",
            "Iteration: 8000; Train Loss: 1.0648 | Perplexity: 2.9003\n",
            "Validation Loss: 2.0737 | Perplexity: 7.9544\n",
            "Iteration: 8020; Train Loss: 1.1414 | Perplexity: 3.1311\n",
            "Iteration: 8040; Train Loss: 0.9717 | Perplexity: 2.6423\n",
            "Iteration: 8060; Train Loss: 1.0106 | Perplexity: 2.7474\n",
            "Iteration: 8080; Train Loss: 0.9731 | Perplexity: 2.6461\n",
            "Iteration: 8100; Train Loss: 1.1106 | Perplexity: 3.0362\n",
            "Iteration: 8120; Train Loss: 1.1345 | Perplexity: 3.1095\n",
            "Iteration: 8140; Train Loss: 0.9720 | Perplexity: 2.6433\n",
            "Iteration: 8160; Train Loss: 0.9206 | Perplexity: 2.5108\n",
            "Iteration: 8180; Train Loss: 0.8854 | Perplexity: 2.4241\n",
            "Iteration: 8200; Train Loss: 0.9904 | Perplexity: 2.6923\n",
            "Iteration: 8220; Train Loss: 0.9846 | Perplexity: 2.6768\n",
            "Iteration: 8240; Train Loss: 1.0880 | Perplexity: 2.9683\n",
            "Iteration: 8260; Train Loss: 1.0953 | Perplexity: 2.9900\n",
            "Iteration: 8280; Train Loss: 0.9350 | Perplexity: 2.5472\n",
            "Iteration: 8300; Train Loss: 0.8769 | Perplexity: 2.4036\n",
            "Iteration: 8320; Train Loss: 0.8594 | Perplexity: 2.3619\n",
            "Iteration: 8340; Train Loss: 0.9215 | Perplexity: 2.5131\n",
            "Iteration: 8360; Train Loss: 0.8494 | Perplexity: 2.3383\n",
            "Iteration: 8380; Train Loss: 0.9152 | Perplexity: 2.4973\n",
            "Iteration: 8400; Train Loss: 0.9040 | Perplexity: 2.4695\n",
            "Iteration: 8420; Train Loss: 0.8535 | Perplexity: 2.3478\n",
            "Iteration: 8440; Train Loss: 0.8727 | Perplexity: 2.3935\n",
            "Iteration: 8460; Train Loss: 1.0583 | Perplexity: 2.8815\n",
            "Iteration: 8480; Train Loss: 0.9903 | Perplexity: 2.6921\n",
            "Iteration: 8500; Train Loss: 0.8244 | Perplexity: 2.2805\n",
            "Iteration: 8520; Train Loss: 0.9593 | Perplexity: 2.6098\n",
            "Iteration: 8540; Train Loss: 1.3598 | Perplexity: 3.8956\n",
            "Iteration: 8560; Train Loss: 0.8107 | Perplexity: 2.2494\n",
            "Iteration: 8580; Train Loss: 0.8440 | Perplexity: 2.3257\n",
            "Iteration: 8600; Train Loss: 0.9136 | Perplexity: 2.4934\n",
            "Iteration: 8620; Train Loss: 0.7343 | Perplexity: 2.0841\n",
            "Iteration: 8640; Train Loss: 0.7797 | Perplexity: 2.1807\n",
            "Iteration: 8660; Train Loss: 0.8523 | Perplexity: 2.3450\n",
            "Iteration: 8680; Train Loss: 0.8474 | Perplexity: 2.3337\n",
            "Iteration: 8700; Train Loss: 0.8838 | Perplexity: 2.4200\n",
            "Iteration: 8720; Train Loss: 0.9531 | Perplexity: 2.5937\n",
            "Iteration: 8740; Train Loss: 0.8529 | Perplexity: 2.3464\n",
            "Iteration: 8760; Train Loss: 0.8410 | Perplexity: 2.3186\n",
            "Iteration: 8780; Train Loss: 0.9008 | Perplexity: 2.4615\n",
            "Iteration: 8800; Train Loss: 0.7998 | Perplexity: 2.2250\n",
            "Iteration: 8820; Train Loss: 0.7913 | Perplexity: 2.2063\n",
            "Iteration: 8840; Train Loss: 0.6980 | Perplexity: 2.0098\n",
            "Iteration: 8860; Train Loss: 0.7948 | Perplexity: 2.2141\n",
            "Iteration: 8880; Train Loss: 0.7596 | Perplexity: 2.1375\n",
            "Iteration: 8900; Train Loss: 0.7993 | Perplexity: 2.2239\n",
            "Iteration: 8920; Train Loss: 0.7486 | Perplexity: 2.1140\n",
            "Iteration: 8940; Train Loss: 0.8620 | Perplexity: 2.3679\n",
            "Iteration: 8960; Train Loss: 0.7405 | Perplexity: 2.0971\n",
            "Iteration: 8980; Train Loss: 0.8401 | Perplexity: 2.3167\n",
            "Iteration: 9000; Train Loss: 0.7104 | Perplexity: 2.0348\n",
            "Validation Loss: 1.7297 | Perplexity: 5.6387\n",
            "Iteration: 9020; Train Loss: 0.8086 | Perplexity: 2.2448\n",
            "Iteration: 9040; Train Loss: 0.6804 | Perplexity: 1.9746\n",
            "Iteration: 9060; Train Loss: 0.5906 | Perplexity: 1.8051\n",
            "Iteration: 9080; Train Loss: 0.6932 | Perplexity: 2.0002\n",
            "Iteration: 9100; Train Loss: 0.7327 | Perplexity: 2.0808\n",
            "Iteration: 9120; Train Loss: 0.6920 | Perplexity: 1.9977\n",
            "Iteration: 9140; Train Loss: 0.7280 | Perplexity: 2.0710\n",
            "Iteration: 9160; Train Loss: 0.7179 | Perplexity: 2.0502\n",
            "Iteration: 9180; Train Loss: 0.6547 | Perplexity: 1.9245\n",
            "Iteration: 9200; Train Loss: 0.6523 | Perplexity: 1.9199\n",
            "Iteration: 9220; Train Loss: 0.6381 | Perplexity: 1.8929\n",
            "Iteration: 9240; Train Loss: 0.6375 | Perplexity: 1.8917\n",
            "Iteration: 9260; Train Loss: 0.6898 | Perplexity: 1.9932\n",
            "Iteration: 9280; Train Loss: 0.7181 | Perplexity: 2.0506\n",
            "Iteration: 9300; Train Loss: 0.6257 | Perplexity: 1.8695\n",
            "Iteration: 9320; Train Loss: 0.6300 | Perplexity: 1.8777\n",
            "Iteration: 9340; Train Loss: 0.6325 | Perplexity: 1.8824\n",
            "Iteration: 9360; Train Loss: 0.7743 | Perplexity: 2.1691\n",
            "Iteration: 9380; Train Loss: 0.7172 | Perplexity: 2.0486\n",
            "Iteration: 9400; Train Loss: 0.6185 | Perplexity: 1.8562\n",
            "Iteration: 9420; Train Loss: 0.5756 | Perplexity: 1.7782\n",
            "Iteration: 9440; Train Loss: 0.5631 | Perplexity: 1.7562\n",
            "Iteration: 9460; Train Loss: 0.5241 | Perplexity: 1.6890\n",
            "Iteration: 9480; Train Loss: 0.6442 | Perplexity: 1.9045\n",
            "Iteration: 9500; Train Loss: 0.5899 | Perplexity: 1.8038\n",
            "Iteration: 9520; Train Loss: 0.5131 | Perplexity: 1.6704\n",
            "Iteration: 9540; Train Loss: 0.5074 | Perplexity: 1.6609\n",
            "Iteration: 9560; Train Loss: 0.4860 | Perplexity: 1.6258\n",
            "Iteration: 9580; Train Loss: 0.5540 | Perplexity: 1.7403\n",
            "Iteration: 9600; Train Loss: 0.5771 | Perplexity: 1.7808\n",
            "Iteration: 9620; Train Loss: 0.5161 | Perplexity: 1.6755\n",
            "Iteration: 9640; Train Loss: 0.4858 | Perplexity: 1.6254\n",
            "Iteration: 9660; Train Loss: 0.6025 | Perplexity: 1.8267\n",
            "Iteration: 9680; Train Loss: 0.6018 | Perplexity: 1.8254\n",
            "Iteration: 9700; Train Loss: 0.5190 | Perplexity: 1.6803\n",
            "Iteration: 9720; Train Loss: 0.6461 | Perplexity: 1.9081\n",
            "Iteration: 9740; Train Loss: 0.5071 | Perplexity: 1.6605\n",
            "Iteration: 9760; Train Loss: 0.6443 | Perplexity: 1.9046\n",
            "Iteration: 9780; Train Loss: 0.5647 | Perplexity: 1.7589\n",
            "Iteration: 9800; Train Loss: 0.5199 | Perplexity: 1.6818\n",
            "Iteration: 9820; Train Loss: 0.4701 | Perplexity: 1.6002\n",
            "Iteration: 9840; Train Loss: 0.5074 | Perplexity: 1.6610\n",
            "Iteration: 9860; Train Loss: 0.4815 | Perplexity: 1.6185\n",
            "Iteration: 9880; Train Loss: 0.5969 | Perplexity: 1.8164\n",
            "Iteration: 9900; Train Loss: 0.4052 | Perplexity: 1.4996\n",
            "Iteration: 9920; Train Loss: 0.5192 | Perplexity: 1.6806\n",
            "Iteration: 9940; Train Loss: 0.5064 | Perplexity: 1.6593\n",
            "Iteration: 9960; Train Loss: 0.5165 | Perplexity: 1.6762\n",
            "Iteration: 9980; Train Loss: 0.5213 | Perplexity: 1.6842\n",
            "Iteration: 10000; Train Loss: 0.5272 | Perplexity: 1.6941\n",
            "Validation Loss: 1.4247 | Perplexity: 4.1567\n",
            "✅ Final checkpoint saved to W&B.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from utils import indexesFromSentence, normalizeString\n",
        "#from config import MAX_LENGTH\n",
        "\n",
        "# Helper to move tensor to correct device\n",
        "def to_device(tensor):\n",
        "    return tensor.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# Safe version that logs unknown words\n",
        "def safe_indexesFromSentence(voc, sentence):\n",
        "    missing = []\n",
        "    indexes = []\n",
        "    for word in sentence.split(\" \"):\n",
        "        if word in voc.word2index:\n",
        "            indexes.append(voc.word2index[word])\n",
        "        else:\n",
        "            missing.append(word)\n",
        "    if missing:\n",
        "        print(f\"Missing words in vocab: {missing}\")\n",
        "    return indexes + [2]  # EOS_token\n",
        "\n",
        "# Greedy decoder\n",
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "\n",
        "        # For LSTM: encoder_hidden is a tuple (h, c)\n",
        "        decoder_hidden = (encoder_hidden[0][:self.decoder.n_layers], encoder_hidden[1][:self.decoder.n_layers])\n",
        "        decoder_input = torch.ones(1, 1, device=input_seq.device, dtype=torch.long) * 1  # SOS_token\n",
        "\n",
        "        all_tokens = torch.zeros([0], device=input_seq.device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=input_seq.device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "\n",
        "        return all_tokens, all_scores\n",
        "\n",
        "# Final evaluation wrapper\n",
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    sentence = normalizeString(sentence)\n",
        "    indexes_batch = [safe_indexesFromSentence(voc, sentence)]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    input_batch = to_device(input_batch)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words"
      ],
      "metadata": {
        "id": "V9VLnR_An44-"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "artifact = wandb.use_artifact(\"abhi1199-city-university-of-london/seq2seqLSTM/final_chatbot_checkpoint:v0\", type=\"model\")\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "checkpoint_path = os.path.join(artifact_dir, \"final_checkpoint.tar\")\n",
        "voc_path = os.path.join(artifact_dir, \"voc.pkl\")\n",
        "\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "with open(voc_path, \"rb\") as f:\n",
        "    voc = pickle.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCOfvPFLXg9-",
        "outputId": "156fea80-7bf2-42a7-c5de-7a6901dc1c51"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact final_chatbot_checkpoint:v0, 1471.71MB. 2 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
            "Done. 0:0:10.4\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = load_glove_embeddings(\n",
        "    voc,\n",
        "    glove_path=\"/content/glove.6B.300d.txt\",\n",
        "    embedding_dim=300,\n",
        "    freeze=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "encoder = LSTMEncoder(\n",
        "    hidden_size=1000,\n",
        "    embedding=embedding,\n",
        "    n_layers=4,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "decoder = LSTMAttnDecoder(\n",
        "    attn_model=\"dot\",\n",
        "    embedding=embedding,\n",
        "    hidden_size=1000,\n",
        "    output_size=voc.num_words,\n",
        "    n_layers=4,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "\n",
        "encoder.load_state_dict(checkpoint[\"encoder_state\"])\n",
        "decoder.load_state_dict(checkpoint[\"decoder_state\"])\n",
        "embedding.load_state_dict(checkpoint[\"embedding_state\"])\n"
      ],
      "metadata": {
        "id": "dW8xfNbZX2Wr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc2f3239-6549-4d87-8693-2cb14f7f2cdf"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe embeddings...\n",
            "Found 9680/9891 words in GloVe.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from evaluate import GreedySearchDecoder, evaluate\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n"
      ],
      "metadata": {
        "id": "OR1Hc7HDOhbi"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    print(\"FinanceBot is ready! Type 'quit' to exit.\")\n",
        "    while True:\n",
        "        input_sentence = input(\"> \")\n",
        "        if input_sentence.lower() in [\"quit\", \"q\"]:\n",
        "            break\n",
        "        try:\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            output_words = [w for w in output_words if w not in [\"EOS\", \"PAD\"]]\n",
        "            print(\"Bot:\", \" \".join(output_words))\n",
        "        except KeyError:\n",
        "            print(\"Oops! Encountered unknown word.\")\n",
        "\n",
        "chat()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seakwdpe7lfK",
        "outputId": "0e99810b-cdd1-40dc-8dee-0cd2a126b002"
      },
      "execution_count": 61,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🤖 FinanceBot is ready! Type 'quit' to exit.\n",
            "> car loan\n",
            "Bot: . price . . .\n",
            "> housing loan enquiry\n",
            "❌ Missing words in vocab: ['enquiry']\n",
            "Bot: the total salary is . . . . .\n",
            "> loan enquiry\n",
            "❌ Missing words in vocab: ['enquiry']\n",
            "Bot:  . . . .\n",
            "> what is inflation\n",
            "Bot: this is change is . . . . .\n",
            "> Can a company block a specific person from buying its stock?\n",
            "Bot: dear spam plan . . .\n",
            "> Do I have to pay a capital gains tax if I rebuy the same stock within 30 days?\n",
            "❌ Missing words in vocab: ['gains', 'rebuy']\n",
            "Bot: yes what did find make the market of start for week ?\n",
            "> Can a credit card company raise my rates for making a large payment?\n",
            "Bot: dear customer . . .\n",
            "> How to motivate young people to save money\n",
            "Bot: money plan to help daily members to save their poor results .\n",
            "> How much should a new graduate with new job put towards a car?\n",
            "Bot: we s learning feedback for break a task at this product .\n",
            "> What are my investment options in real estate?\n",
            "Bot: here are reasons that have have equity that of months\n",
            "> What's the difference between Term and Whole Life insurance?\n",
            "Bot: question . for a answer due !\n",
            "> How to split stock earnings?\n",
            "❌ Missing words in vocab: ['earnings']\n",
            "Bot: these cost is costs . . . .\n",
            "> Is it ever a good idea to close credit cards?\n",
            "Bot: no new matter it is a lot to week . .\n",
            "> Would I need to keep track of 1099s?\n",
            "Bot: i you take to get this information or question . .\n",
            "> In what cases can a business refuse to take cash?\n",
            "Bot: there are a few information to need to need .\n",
            "> How can I make a profit by selling a stock short?\n",
            "Bot: no unlikely unbeatable . . . . .\n",
            "> what is orange\n",
            "Bot: is is not than large while while and yellow . .\n",
            "> an apple a day keeps doctor away\n",
            "Bot: a family a day a day don be enjoy at my music .\n",
            "> Will I be paid dividends if I own shares?\n",
            "Bot: the boutique negative that that be able to buy . .\n",
            "> What percent of my salary should I save?\n",
            "Bot: no it of get it of you you must find your goal .\n",
            "> What are the options for a 19-year-old college student who only has about $1000?\n",
            "Bot: school old time old old old years . . .\n",
            "> How to tell if an option is expensive\tAn option, by definition, is a guess about the future value of the stock. If you guess too aggressively, you lose the purchase price of the option; if you guess too conservatively, you may not take the option or may not gain as much as you might have. You need to figure out what you expect to happen, and how confident you are about it, against the cost of taking the option -- and be reasonably confident that the change in the stock's value will be at least large enough to cover the cost of buying into the game. Opinion: Unless you're comfortable with expectation values and bell curves around them, it's significantly easier to lose money on options than to profit on them. And I'm not convinced that even statisticians can really do this well. I've always been told that the best use for options is hedging an investment you've already made; treating them as your primary bet is gambling, not investment. How to dollar-cost-average with a large amount of money in a savings account?\tDCA is not 10%/day over 10 days. If I read the objective correctly, I'd suggest about a 5 year plan. It's difficult to avoid the issue of market timing. And any observation I'd make about the relative valuation of the market would be opinion. By this I mean, some are saying that PE/10 which Nobel prize winner Robert Schiller made well known, if not popular, shows we are pretty high. Others are suggesting the current PE is appropriate given the near zero rate of borrowing.  Your income puts long term gains at zero under current tax code. Short term are at your marginal rate.  I would caution not to let the tax tail wag the investing dog. The fellow that makes too many buy/sell decisions based on his taxes is likely to lag he who followed his overall allocation goals. Double-entry accounting: how to keep track of mortgage installments as expenses?\tThe best thing for you to do will be to start using the Cash Flow report instead of the Income and Expense report. Go to Reports -> Income and Expense -> Cash Flow Once the report is open, open the edit window and open the Accounts tab.  There, choose your various cash accounts (checking, saving, etc.).  In the General tab, choose the reporting period.  (And then save the report settings so you don't need to go hunting for your cash accounts each time.) GnuCash will display for you all the inflows and outflows of money, which appears to be what you really want. Though GnuCash doesn't present the Cash Flow in a way that matches United States accounting rules (with sections for operating, investing, and financial cash flows separated), it is certainly fine for your personal use. If you want the total payment to show up as one line on the Cash Flow report, you will need to book the accrual of interest and the payment to the mortgage bank as two separate entries. Normal entry for mortgage payments (which shows up as a line for mortgage and a line for interest on your Cash Flow): Pair of entries to make full mortgage payment show up as one line on Cash Flow: Entry #1:  Interest accrual Entry #2:  Full mortgage payment (Tested in GnuCash 2.6.1) What assets would be valuable in a post-apocalyptic scenario?\tBullets, canned goods, and farm supplies that don't need gas (e.g. seed, feed, plows). Are my purchases of stock, mutual funds, ETF's, and commodities investing, or speculation?\t\"Every investment comes with a risk. There is also a bit of speculation involved. In there is an anticipation that one expects the value to go up in normal course of events. By your definition \"\"If I buy this equipment, I could produce more widgets, or sell more widgets,\"\" as an investment. Here again there is an anticipation that the widgets you sell will give you more return. If you are investing in stock/share, you are essentially holding a small portion of value in company and to that extent you are owining some equipment that is producing some widget .... Hence when you are purchasing Stocks, it would be looked as investment if you have done your home work and have a good plan of how you want to invest along with weiging the risk involved. However if you are investing only for the purpose of making quick bucks following so called hot tips, then you are not investing but speculating.\" Am I understanding buying options on stock correctly\n",
            "❌ Missing words in vocab: ['aggressively', 'conservatively', 'reasonably', 'unless', 'expectation', 'curves', 'convinced', 'statisticians', 'hedging', 'bet', 'gambling', 'dca', 'timing', 'valuation', 'pe', 'schiller', 'suggesting', 'pe', 'borrowing', 'puts', 'gains', 'marginal', 'wag', 'fellow', 'lag', 'allocation', 'installments', 'tab', 'tab', 'hunting', 'inflows', 'outflows', 'separated', 'certainly', 'accrual', 'separate', 'entries', 'entries', 'accrual', 'bullets', 'canned', 'plows', 'mutual', 'etf', 'commodities', 'speculation', 'speculation', 'expects', 'widgets', 'widgets', 'widgets', 'essentially', 'extent', 'owining', 'widget', 'hence', 'weiging', 'bucks', 'speculating']\n",
            "Bot: someone . it s . . it can be challenging must today .\n",
            "> Risk and return always go hand by hand\n",
            "Bot: they forget up their dinner . . .\n",
            "> The best investment at this stage is a good, easy to understand but thorough book on finance\n",
            "Bot: it is an exhilarating job with work on an job development .\n",
            "> Would it make sense to take a loan from a relative to pay off student loans?\n",
            "Bot: money pay pay pay a salary needed . .\n",
            "> How is money actually made from the buying or selling of options?\n",
            "Bot: minimum cost has money of . . . .\n",
            "> Is it wise to switch investment strategy frequently?\n",
            "Bot: it can not committed to money for a own environment .\n",
            "> Why are currency forwards needed?\t\n",
            "❌ Missing words in vocab: ['forwards']\n",
            "Bot: question . . . .\n",
            "> How can I find a list of self-select stocks & shares ISA providers?\n",
            "❌ Missing words in vocab: ['isa', 'providers']\n",
            "Bot: there are a list of cup egg and tomatoes .\n",
            "> how to make more money\n",
            "Bot: we ought to money money . . . .\n",
            "> how to plan my studies\n",
            "Bot: you you to start on my assignment . . .\n",
            "> how to start my investment journey\n",
            "Bot: dear morning . a pleasure or meet you .\n",
            "> what is london\n",
            "Bot: question . . . .\n",
            "> what is bank\n",
            "Bot: investment is . . . .\n",
            "> what is investment\n",
            "Bot: investment . . . . .\n",
            "> what is credit card\n",
            "Bot: the first price is increasing . . . .\n",
            "> inflation is\n",
            "Bot: . . . . .\n",
            "> car loan\n",
            "Bot: . price . . .\n",
            "> car\n",
            "Bot: . . . . . .\n",
            "> car loan needs amount\n",
            "Bot: gross cost . . .\n",
            "> i want to buy a car. should i buy or not\n",
            "Bot: i bought a car if i had enough buy some buy .\n",
            "> quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k3Np8s45Y1oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # i want to buy a car. should i buy or not\n",
        "\n",
        " # What percent of my salary should I save?\n",
        "\n",
        " # Is it wise to switch investment strategy frequently?\n",
        "\n",
        " # The best investment at this stage is a good, easy to understand but thorough book on finance\n",
        "\n",
        " # How to motivate young people to save money\n",
        "\n",
        " # How much should a new graduate with new job put towards a car?\n",
        "\n",
        " # What are my investment options in real estate?\n",
        "\n",
        " # Is it ever a good idea to close credit cards?\n",
        "\n",
        "\n",
        " # Would I need to keep track of 1099s?\n",
        "\n",
        " # Will I be paid dividends if I own shares?\n"
      ],
      "metadata": {
        "id": "4eNAfbacYu_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w6pMoSDGSvue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}