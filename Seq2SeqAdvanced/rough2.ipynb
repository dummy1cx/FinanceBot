{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iV6w5ZI4ZgvA"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fy7PqHfgZ15l"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7YIfAbPZcNHm"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jrIf6kYNcqKs"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import itertools\n",
        "import torch\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "\n",
        "# Default word tokens\n",
        "PAD_token = 0  # Used for padding short sentences\n",
        "SOS_token = 1  # Start-of-sentence token\n",
        "EOS_token = 2  # End-of-sentence token\n",
        "\n",
        "class Voc:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.trimmed = False\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3  # Count default tokens\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.num_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.num_words] = word\n",
        "            self.num_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1\n",
        "\n",
        "    def trim(self, min_count):\n",
        "        if self.trimmed:\n",
        "            return\n",
        "        self.trimmed = True\n",
        "\n",
        "        keep_words = [k for k, v in self.word2count.items() if v >= min_count]\n",
        "\n",
        "        print('keep_words {} / {} = {:.4f}'.format(\n",
        "            len(keep_words), len(self.word2index), len(keep_words) / len(self.word2index)\n",
        "        ))\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {PAD_token: \"PAD\", SOS_token: \"SOS\", EOS_token: \"EOS\"}\n",
        "        self.num_words = 3\n",
        "\n",
        "        for word in keep_words:\n",
        "            self.addWord(word)\n",
        "\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    s = re.sub(r\"\\s+\", r\" \", s).strip()\n",
        "    return s\n",
        "\n",
        "def indexesFromSentence(voc, sentence):\n",
        "    return [voc.word2index[word] for word in sentence.split(' ')] + [EOS_token]\n",
        "\n",
        "def zeroPadding(l, fillvalue=PAD_token):\n",
        "    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n",
        "\n",
        "def binaryMatrix(l, value=PAD_token):\n",
        "    m = []\n",
        "    for seq in l:\n",
        "        m.append([0 if token == PAD_token else 1 for token in seq])\n",
        "    return m\n",
        "\n",
        "def inputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, lengths\n",
        "\n",
        "def outputVar(l, voc):\n",
        "    indexes_batch = [indexesFromSentence(voc, sentence) for sentence in l]\n",
        "    max_target_len = max([len(indexes) for indexes in indexes_batch])\n",
        "    padList = zeroPadding(indexes_batch)\n",
        "    mask = binaryMatrix(padList)\n",
        "    mask = torch.BoolTensor(mask)\n",
        "    padVar = torch.LongTensor(padList)\n",
        "    return padVar, mask, max_target_len\n",
        "\n",
        "def batch2TrainData(voc, pair_batch):\n",
        "    pair_batch.sort(key=lambda x: len(x[0].split(\" \")), reverse=True)\n",
        "    input_batch, output_batch = zip(*pair_batch)\n",
        "    inp, lengths = inputVar(input_batch, voc)\n",
        "    output, mask, max_target_len = outputVar(output_batch, voc)\n",
        "    return inp, lengths, output, mask, max_target_len\n",
        "\n",
        "# GloVe integration\n",
        "def load_glove_embeddings(voc, glove_path, embedding_dim=300, freeze=True):\n",
        "    print(\"Loading GloVe embeddings...\")\n",
        "    glove = {}\n",
        "    with open(glove_path, encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            parts = line.strip().split()\n",
        "            word = parts[0]\n",
        "            vector = np.array(parts[1:], dtype=np.float32)\n",
        "            glove[word] = vector\n",
        "\n",
        "    embedding_matrix = np.random.normal(0, 1, (voc.num_words, embedding_dim)).astype(np.float32)\n",
        "    found = 0\n",
        "    for word, idx in voc.word2index.items():\n",
        "        if word in glove:\n",
        "            embedding_matrix[idx] = glove[word]\n",
        "            found += 1\n",
        "\n",
        "    print(f\"Found {found}/{voc.num_words} words in GloVe.\")\n",
        "    tensor = torch.tensor(embedding_matrix)\n",
        "    return nn.Embedding.from_pretrained(tensor, freeze=freeze)\n"
      ],
      "metadata": {
        "id": "sjqcL4SycqM6"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import os\n",
        "import random\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "#from utils import Voc, normalizeString, batch2TrainData, load_glove_embeddings\n",
        "\n",
        "MAX_LENGTH = 15\n",
        "\n",
        "class ChatDataset(Dataset):\n",
        "    def __init__(self, pairs, voc):\n",
        "        self.pairs = pairs\n",
        "        self.voc = voc\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.pairs[idx]\n",
        "\n",
        "def readVocs(datafile, corpus_name):\n",
        "    print(\"Reading lines...\")\n",
        "    lines = open(datafile, encoding='utf-8').read().strip().split('\\n')\n",
        "    pairs = []\n",
        "    for l in lines:\n",
        "        parts = l.split('\\t')\n",
        "        if len(parts) == 2:\n",
        "            pairs.append([normalizeString(parts[0]), normalizeString(parts[1])])\n",
        "\n",
        "    voc = Voc(corpus_name)\n",
        "    return voc, pairs\n",
        "\n",
        "def filterPair(p):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    return [pair for pair in pairs if filterPair(pair)]\n",
        "\n",
        "def loadPrepareData(corpus, corpus_name, datafile, save_dir):\n",
        "    print(\"Start preparing training data ...\")\n",
        "    voc, pairs = readVocs(datafile, corpus_name)\n",
        "    print(\"Read {!s} sentence pairs\".format(len(pairs)))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to {!s} sentence pairs\".format(len(pairs)))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        voc.addSentence(pair[0])\n",
        "        voc.addSentence(pair[1])\n",
        "    print(\"Counted words:\", voc.num_words)\n",
        "    return voc, pairs\n",
        "\n",
        "def trimRareWords(voc, pairs, MIN_COUNT):\n",
        "    voc.trim(MIN_COUNT)\n",
        "    keep_pairs = []\n",
        "    for pair in pairs:\n",
        "        input_sentence, output_sentence = pair\n",
        "        keep_input = all(word in voc.word2index for word in input_sentence.split(' '))\n",
        "        keep_output = all(word in voc.word2index for word in output_sentence.split(' '))\n",
        "        if keep_input and keep_output:\n",
        "            keep_pairs.append(pair)\n",
        "\n",
        "    print(\"Trimmed from {} pairs to {}, {:.4f} of total\".format(len(pairs), len(keep_pairs), len(keep_pairs) / len(pairs)))\n",
        "    return keep_pairs\n",
        "\n",
        "def collate_fn(batch, voc):\n",
        "    return batch2TrainData(voc, batch)\n",
        "\n",
        "def split_dataset(pairs, test_size=0.1, random_state=42):\n",
        "    train_pairs, val_pairs = train_test_split(pairs, test_size=test_size, random_state=random_state)\n",
        "    print(f\"Split {len(pairs)} pairs into {len(train_pairs)} train and {len(val_pairs)} validation\")\n",
        "    return train_pairs, val_pairs\n",
        "\n",
        "def get_dataloader(pairs, voc, batch_size=64, shuffle=True):\n",
        "    dataset = ChatDataset(pairs, voc)\n",
        "    return DataLoader(dataset, batch_size=batch_size, shuffle=shuffle, collate_fn=lambda x: collate_fn(x, voc))"
      ],
      "metadata": {
        "id": "FuzQ8lODcqPQ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Encoder with LSTM\n",
        "class LSTMEncoder(nn.Module):\n",
        "    def __init__(self, hidden_size, embedding, n_layers=1, dropout=0):\n",
        "        super(LSTMEncoder, self).__init__()\n",
        "        self.n_layers = n_layers\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = embedding\n",
        "        self.lstm = nn.LSTM(embedding.embedding_dim, hidden_size, n_layers,\n",
        "                            dropout=(0 if n_layers == 1 else dropout),\n",
        "                            bidirectional=True)\n",
        "\n",
        "    def forward(self, input_seq, input_lengths, hidden=None):\n",
        "        embedded = self.embedding(input_seq)\n",
        "        self.lstm.flatten_parameters()  # Fix for RNN warning\n",
        "        packed = nn.utils.rnn.pack_padded_sequence(embedded, input_lengths, enforce_sorted=False)\n",
        "        outputs, hidden = self.lstm(packed, hidden)\n",
        "        outputs, _ = nn.utils.rnn.pad_packed_sequence(outputs)\n",
        "        outputs = outputs[:, :, :self.hidden_size] + outputs[:, :, self.hidden_size:]\n",
        "        return outputs, hidden\n",
        "\n",
        "# Attention mechanism\n",
        "class Attn(nn.Module):\n",
        "    def __init__(self, method, hidden_size):\n",
        "        super(Attn, self).__init__()\n",
        "        self.method = method\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if self.method not in ['dot', 'general', 'concat']:\n",
        "            raise ValueError(f\"{self.method} is not a valid attention method.\")\n",
        "\n",
        "        if self.method == 'general':\n",
        "            self.attn = nn.Linear(hidden_size, hidden_size)\n",
        "        elif self.method == 'concat':\n",
        "            self.attn = nn.Linear(hidden_size * 2, hidden_size)\n",
        "            self.v = nn.Parameter(torch.FloatTensor(hidden_size))\n",
        "\n",
        "    def dot_score(self, hidden, encoder_output):\n",
        "        return torch.sum(hidden * encoder_output, dim=2)\n",
        "\n",
        "    def general_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(encoder_output)\n",
        "        return torch.sum(hidden * energy, dim=2)\n",
        "\n",
        "    def concat_score(self, hidden, encoder_output):\n",
        "        energy = self.attn(torch.cat((hidden.expand(encoder_output.size(0), -1, -1),\n",
        "                                      encoder_output), 2)).tanh()\n",
        "        return torch.sum(self.v * energy, dim=2)\n",
        "\n",
        "    def forward(self, hidden, encoder_outputs):\n",
        "        if self.method == 'general':\n",
        "            attn_energies = self.general_score(hidden, encoder_outputs)\n",
        "        elif self.method == 'concat':\n",
        "            attn_energies = self.concat_score(hidden, encoder_outputs)\n",
        "        else:  # dot\n",
        "            attn_energies = self.dot_score(hidden, encoder_outputs)\n",
        "\n",
        "        attn_energies = attn_energies.t()\n",
        "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
        "\n",
        "# Decoder with Attention and LSTM\n",
        "class LSTMAttnDecoder(nn.Module):\n",
        "    def __init__(self, attn_model, embedding, hidden_size, output_size, n_layers=1, dropout=0.1):\n",
        "        super(LSTMAttnDecoder, self).__init__()\n",
        "        self.attn_model = attn_model\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.n_layers = n_layers\n",
        "        self.embedding = embedding\n",
        "\n",
        "        self.embedding_dropout = nn.Dropout(dropout)\n",
        "        self.lstm = nn.LSTM(embedding.embedding_dim, hidden_size, n_layers,\n",
        "                            dropout=(0 if n_layers == 1 else dropout))\n",
        "        self.concat = nn.Linear(hidden_size * 2, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.attn = Attn(attn_model, hidden_size)\n",
        "\n",
        "    def forward(self, input_step, last_hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input_step)\n",
        "        embedded = self.embedding_dropout(embedded)\n",
        "        self.lstm.flatten_parameters()  # Fix for RNN warning\n",
        "        rnn_output, hidden = self.lstm(embedded, last_hidden)\n",
        "        attn_weights = self.attn(rnn_output, encoder_outputs)\n",
        "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))\n",
        "        rnn_output = rnn_output.squeeze(0)\n",
        "        context = context.squeeze(1)\n",
        "        concat_input = torch.cat((rnn_output, context), 1)\n",
        "        concat_output = torch.tanh(self.concat(concat_input))\n",
        "        output = self.out(concat_output)\n",
        "        output = F.softmax(output, dim=1)\n",
        "        return output, hidden\n"
      ],
      "metadata": {
        "id": "OyqmVZ2ncqRV"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Initializes wandb api key = \"delete\"\n",
        "\n",
        "def init_wandb(project_name=\"seq2seqLSTM_last\", config=None):\n",
        "    if config is None:\n",
        "        config = {\n",
        "            \"model_name\": \"deep_lstm_seq2seq\",\n",
        "            \"attn_model\": \"dot\",\n",
        "            \"embedding\": \"glove.6B.300d\",\n",
        "            \"embedding_dim\": 300,  # match paper\n",
        "            \"freeze_embeddings\": True,\n",
        "            \"hidden_size\": 1000,  # deep hidden state\n",
        "            \"encoder_n_layers\": 3,\n",
        "            \"decoder_n_layers\": 3,\n",
        "            \"dropout\": 0.1,\n",
        "            \"batch_size\": 64,  # match paper\n",
        "            \"learning_rate\": 0.0001,  # fixed LR\n",
        "            \"decoder_learning_ratio\": 1.0,  # same as encoder\n",
        "            \"teacher_forcing_ratio\": 0.9,\n",
        "            \"clip\": 5.0,  # gradient norm clipping\n",
        "            \"n_iteration\": 4000,  # longer run\n",
        "            \"print_every\": 20,\n",
        "            \"save_every\": 1000\n",
        "        }\n",
        "\n",
        "    wandb.init(project=project_name, config=config)\n",
        "    return wandb.config\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2XuxlUX2fvpj"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0LUCUrC_dRWy"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import wandb\n",
        "#from utils import Voc, load_glove_embeddings\n",
        "#from dataset import loadPrepareData, trimRareWords, split_dataset, get_dataloader\n",
        "#from models_seq2seq import LSTMEncoder, LSTMAttnDecoder\n",
        "#from wandb_config import init_wandb\n",
        "from datetime import datetime\n",
        "\n",
        "dev_config = init_wandb()\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "datafile = \"/content/formatted_pairs.txt\"\n",
        "save_dir = os.path.join(\"data\", \"save\")\n",
        "corpus_name = \"custom\"\n",
        "voc, pairs = loadPrepareData(\"data\", corpus_name, datafile, save_dir)\n",
        "pairs = trimRareWords(voc, pairs, MIN_COUNT=dev_config.MIN_COUNT if \"MIN_COUNT\" in dev_config else 3)\n",
        "\n",
        "train_pairs, val_pairs = split_dataset(pairs, test_size=0.1)\n",
        "\n",
        "train_loader = get_dataloader(train_pairs, voc, batch_size=dev_config.batch_size)\n",
        "val_loader = get_dataloader(val_pairs, voc, batch_size=dev_config.batch_size)\n",
        "\n",
        "embedding = load_glove_embeddings(\n",
        "    voc,\n",
        "    glove_path=\"/content/glove.6B.300d.txt\",\n",
        "    embedding_dim=dev_config.embedding_dim,\n",
        "    freeze=dev_config.freeze_embeddings\n",
        ")\n",
        "\n",
        "encoder = LSTMEncoder(dev_config.hidden_size, embedding, dev_config.encoder_n_layers, dev_config.dropout).to(device)\n",
        "decoder = LSTMAttnDecoder(dev_config.attn_model, embedding, dev_config.hidden_size, voc.num_words,\n",
        "                          dev_config.decoder_n_layers, dev_config.dropout).to(device)\n",
        "\n",
        "encoder_optimizer = optim.Adam(encoder.parameters(), lr=dev_config.learning_rate)\n",
        "decoder_optimizer = optim.Adam(decoder.parameters(), lr=dev_config.learning_rate * dev_config.decoder_learning_ratio)\n",
        "\n",
        "def save_checkpoint_tar(voc, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, iteration, loss, save_path=\"checkpoint.tar\"):\n",
        "    checkpoint = {\n",
        "        'iteration': iteration,\n",
        "        'encoder_state': encoder.state_dict(),\n",
        "        'decoder_state': decoder.state_dict(),\n",
        "        'embedding_state': embedding.state_dict(),\n",
        "        'encoder_optimizer_state': encoder_optimizer.state_dict(),\n",
        "        'decoder_optimizer_state': decoder_optimizer.state_dict(),\n",
        "        'voc_dict': voc.__dict__,\n",
        "        'loss': loss\n",
        "    }\n",
        "    torch.save(checkpoint, save_path)\n",
        "    with open(\"voc.pkl\", \"wb\") as f:\n",
        "        pickle.dump(voc, f)\n",
        "\n",
        "def log_artifacts_to_wandb(tar_path=\"checkpoint.tar\", voc_path=\"voc.pkl\", artifact_name=\"chatbot_model\"):\n",
        "    artifact = wandb.Artifact(artifact_name, type=\"model\")\n",
        "    artifact.add_file(tar_path)\n",
        "    artifact.add_file(voc_path)\n",
        "    wandb.log_artifact(artifact)\n",
        "\n",
        "def maskNLLLoss(inp, target, mask):\n",
        "    nTotal = mask.sum()\n",
        "    crossEntropy = -torch.log(torch.gather(inp, 1, target.view(-1, 1)).squeeze(1))\n",
        "    loss = crossEntropy.masked_select(mask).mean()\n",
        "    return loss, nTotal.item()\n",
        "\n",
        "def train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "          encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, clip):\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_variable = input_variable.to(device)\n",
        "    target_variable = target_variable.to(device)\n",
        "    mask = mask.to(device)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    current_batch_size = input_variable.size(1)\n",
        "\n",
        "    loss = 0\n",
        "    print_losses = []\n",
        "    n_totals = 0\n",
        "\n",
        "    encoder_outputs, (encoder_hidden, encoder_cell) = encoder(input_variable, lengths)\n",
        "    decoder_input = torch.LongTensor([[1 for _ in range(current_batch_size)]]).to(device)\n",
        "    decoder_hidden = (encoder_hidden[:decoder.n_layers], encoder_cell[:decoder.n_layers])\n",
        "\n",
        "    use_teacher_forcing = True if torch.rand(1).item() < dev_config.teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_input = target_variable[t].view(1, -1)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "    else:\n",
        "        for t in range(max_target_len):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            _, topi = decoder_output.topk(1)\n",
        "            decoder_input = torch.LongTensor([[topi[i][0] for i in range(current_batch_size)]]).to(device)\n",
        "            mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "            loss += mask_loss\n",
        "            print_losses.append(mask_loss.item() * nTotal)\n",
        "            n_totals += nTotal\n",
        "\n",
        "    loss.backward()\n",
        "    _ = nn.utils.clip_grad_norm_(encoder.parameters(), dev_config.clip)\n",
        "    _ = nn.utils.clip_grad_norm_(decoder.parameters(), dev_config.clip)\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return sum(print_losses) / n_totals\n",
        "\n",
        "def evaluate_loss(val_loader, encoder, decoder, embedding):\n",
        "    encoder.eval()\n",
        "    decoder.eval()\n",
        "    total_loss = 0\n",
        "    total_count = 0\n",
        "    with torch.no_grad():\n",
        "        for input_variable, lengths, target_variable, mask, max_target_len in val_loader:\n",
        "            input_variable = input_variable.to(device)\n",
        "            target_variable = target_variable.to(device)\n",
        "            mask = mask.to(device)\n",
        "            lengths = lengths.to(\"cpu\")\n",
        "\n",
        "            current_batch_size = input_variable.size(1)\n",
        "\n",
        "            encoder_outputs, (encoder_hidden, encoder_cell) = encoder(input_variable, lengths)\n",
        "            decoder_input = torch.LongTensor([[1 for _ in range(current_batch_size)]]).to(device)\n",
        "            decoder_hidden = (encoder_hidden[:decoder.n_layers], encoder_cell[:decoder.n_layers])\n",
        "\n",
        "            for t in range(max_target_len):\n",
        "                decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "                decoder_input = target_variable[t].view(1, -1)\n",
        "                mask_loss, nTotal = maskNLLLoss(decoder_output, target_variable[t], mask[t])\n",
        "                total_loss += mask_loss.item() * nTotal\n",
        "                total_count += nTotal\n",
        "    encoder.train()\n",
        "    decoder.train()\n",
        "    return total_loss / total_count\n",
        "\n",
        "print(\"\\nStarting training...\")\n",
        "train_iter = iter(train_loader)\n",
        "for iteration in range(1, dev_config.n_iteration + 1):\n",
        "    try:\n",
        "        batch = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        batch = next(train_iter)\n",
        "\n",
        "    input_variable, lengths, target_variable, mask, max_target_len = batch\n",
        "    train_loss = train(input_variable, lengths, target_variable, mask, max_target_len,\n",
        "                       encoder, decoder, embedding, encoder_optimizer, decoder_optimizer,\n",
        "                       dev_config.clip)\n",
        "\n",
        "    perplexity = math.exp(train_loss)\n",
        "    wandb.log({\n",
        "        \"train_loss\": train_loss,\n",
        "        \"train_perplexity\": perplexity,\n",
        "        \"iteration\": iteration\n",
        "    })\n",
        "\n",
        "    if iteration % dev_config.print_every == 0:\n",
        "        print(\"Iteration: {}; Train Loss: {:.4f} | Perplexity: {:.4f}\".format(iteration, train_loss, perplexity))\n",
        "\n",
        "    if iteration % dev_config.save_every == 0:\n",
        "        val_loss = evaluate_loss(val_loader, encoder, decoder, embedding)\n",
        "        val_perplexity = math.exp(val_loss)\n",
        "        wandb.log({\n",
        "            \"val_loss\": val_loss,\n",
        "            \"val_perplexity\": val_perplexity\n",
        "        })\n",
        "        print(\"Validation Loss: {:.4f} | Perplexity: {:.4f}\".format(val_loss, val_perplexity))\n",
        "\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        save_checkpoint_tar(voc, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, iteration, val_loss, f\"checkpoint_{timestamp}.tar\")\n",
        "        log_artifacts_to_wandb(f\"checkpoint_{timestamp}.tar\", \"voc.pkl\", f\"chatbot_checkpoint_{iteration}\")\n",
        "\n",
        "save_checkpoint_tar(voc, encoder, decoder, embedding, encoder_optimizer, decoder_optimizer, iteration, train_loss, \"final_checkpoint.tar\")\n",
        "log_artifacts_to_wandb(\"final_checkpoint.tar\", \"voc.pkl\", \"final_chatbot_checkpoint\")\n",
        "print(\"Final checkpoint saved to W&B.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bF8YjkdfgEc9",
        "outputId": "282e0335-d403-49b6-81c1-f16483b79293"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>iteration</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▄▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▇▇▇▆▇▆▆▆▆▆▆▅▅▅▅▅▄▄▄▃▃▄▄▄▃▃▂▂▃▂▂▂▂▂▂▂▁▁▁</td></tr><tr><td>train_perplexity</td><td>█▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_loss</td><td>█▅▃▁</td></tr><tr><td>val_perplexity</td><td>█▄▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>iteration</td><td>4000</td></tr><tr><td>train_loss</td><td>3.41531</td></tr><tr><td>train_perplexity</td><td>30.42635</td></tr><tr><td>val_loss</td><td>3.96261</td></tr><tr><td>val_perplexity</td><td>52.5944</td></tr></table><br/></div></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">helpful-plasma-9</strong> at: <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM_last/runs/7qhkq5yn' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM_last/runs/7qhkq5yn</a><br> View project at: <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM_last' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM_last</a><br>Synced 5 W&B file(s), 0 media file(s), 10 artifact file(s) and 0 other file(s)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Find logs at: <code>./wandb/run-20250418_155111-7qhkq5yn/logs</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.9"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250418_155759-lagny298</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM_last/runs/lagny298' target=\"_blank\">young-sound-10</a></strong> to <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM_last' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM_last' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM_last</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM_last/runs/lagny298' target=\"_blank\">https://wandb.ai/abhi1199-city-university-of-london/seq2seqLSTM_last/runs/lagny298</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start preparing training data ...\n",
            "Reading lines...\n",
            "Read 129656 sentence pairs\n",
            "Trimmed to 43827 sentence pairs\n",
            "Counting words...\n",
            "Counted words: 16561\n",
            "keep_words 9888 / 16558 = 0.5972\n",
            "Trimmed from 43827 pairs to 34583, 0.7891 of total\n",
            "Split 34583 pairs into 31124 train and 3459 validation\n",
            "Loading GloVe embeddings...\n",
            "Found 9680/9891 words in GloVe.\n",
            "\n",
            "Starting training...\n",
            "Iteration: 20; Train Loss: 7.7017 | Perplexity: 2212.1821\n",
            "Iteration: 40; Train Loss: 6.4055 | Perplexity: 605.1360\n",
            "Iteration: 60; Train Loss: 6.2239 | Perplexity: 504.6613\n",
            "Iteration: 80; Train Loss: 5.9637 | Perplexity: 389.0337\n",
            "Iteration: 100; Train Loss: 6.0794 | Perplexity: 436.7752\n",
            "Iteration: 120; Train Loss: 6.0192 | Perplexity: 411.2429\n",
            "Iteration: 140; Train Loss: 6.0679 | Perplexity: 431.7604\n",
            "Iteration: 160; Train Loss: 6.0850 | Perplexity: 439.2021\n",
            "Iteration: 180; Train Loss: 5.9395 | Perplexity: 379.7597\n",
            "Iteration: 200; Train Loss: 6.1850 | Perplexity: 485.4146\n",
            "Iteration: 220; Train Loss: 5.8627 | Perplexity: 351.6789\n",
            "Iteration: 240; Train Loss: 6.1752 | Perplexity: 480.7016\n",
            "Iteration: 260; Train Loss: 5.9739 | Perplexity: 393.0474\n",
            "Iteration: 280; Train Loss: 5.8977 | Perplexity: 364.2118\n",
            "Iteration: 300; Train Loss: 5.9720 | Perplexity: 392.2883\n",
            "Iteration: 320; Train Loss: 5.9638 | Perplexity: 389.0807\n",
            "Iteration: 340; Train Loss: 5.8648 | Perplexity: 352.4179\n",
            "Iteration: 360; Train Loss: 5.7225 | Perplexity: 305.6592\n",
            "Iteration: 380; Train Loss: 5.8544 | Perplexity: 348.7809\n",
            "Iteration: 400; Train Loss: 5.7673 | Perplexity: 319.6768\n",
            "Iteration: 420; Train Loss: 5.8638 | Perplexity: 352.0692\n",
            "Iteration: 440; Train Loss: 5.7638 | Perplexity: 318.5704\n",
            "Iteration: 460; Train Loss: 5.6045 | Perplexity: 271.6533\n",
            "Iteration: 480; Train Loss: 5.7167 | Perplexity: 303.9053\n",
            "Iteration: 500; Train Loss: 5.5551 | Perplexity: 258.5603\n",
            "Iteration: 520; Train Loss: 5.6365 | Perplexity: 280.4718\n",
            "Iteration: 540; Train Loss: 5.5566 | Perplexity: 258.9432\n",
            "Iteration: 560; Train Loss: 5.6632 | Perplexity: 288.0674\n",
            "Iteration: 580; Train Loss: 5.4641 | Perplexity: 236.0679\n",
            "Iteration: 600; Train Loss: 5.5426 | Perplexity: 255.3326\n",
            "Iteration: 620; Train Loss: 5.5957 | Perplexity: 269.2702\n",
            "Iteration: 640; Train Loss: 5.4703 | Perplexity: 237.5372\n",
            "Iteration: 660; Train Loss: 5.3726 | Perplexity: 215.4156\n",
            "Iteration: 680; Train Loss: 5.3337 | Perplexity: 207.2123\n",
            "Iteration: 700; Train Loss: 5.5720 | Perplexity: 262.9494\n",
            "Iteration: 720; Train Loss: 5.3582 | Perplexity: 212.3472\n",
            "Iteration: 740; Train Loss: 5.5681 | Perplexity: 261.9427\n",
            "Iteration: 760; Train Loss: 5.2683 | Perplexity: 194.0909\n",
            "Iteration: 780; Train Loss: 5.2148 | Perplexity: 183.9753\n",
            "Iteration: 800; Train Loss: 5.4767 | Perplexity: 239.0458\n",
            "Iteration: 820; Train Loss: 5.5385 | Perplexity: 254.2972\n",
            "Iteration: 840; Train Loss: 5.0687 | Perplexity: 158.9710\n",
            "Iteration: 860; Train Loss: 5.2764 | Perplexity: 195.6695\n",
            "Iteration: 880; Train Loss: 5.4229 | Perplexity: 226.5427\n",
            "Iteration: 900; Train Loss: 5.1124 | Perplexity: 166.0611\n",
            "Iteration: 920; Train Loss: 5.3342 | Perplexity: 207.2987\n",
            "Iteration: 940; Train Loss: 5.4410 | Perplexity: 230.6839\n",
            "Iteration: 960; Train Loss: 5.1891 | Perplexity: 179.3053\n",
            "Iteration: 980; Train Loss: 5.2072 | Perplexity: 182.5870\n",
            "Iteration: 1000; Train Loss: 5.3391 | Perplexity: 208.3323\n",
            "Validation Loss: 5.2437 | Perplexity: 189.3607\n",
            "Iteration: 1020; Train Loss: 5.1161 | Perplexity: 166.6807\n",
            "Iteration: 1040; Train Loss: 5.1944 | Perplexity: 180.2603\n",
            "Iteration: 1060; Train Loss: 4.8577 | Perplexity: 128.7301\n",
            "Iteration: 1080; Train Loss: 4.9964 | Perplexity: 147.8783\n",
            "Iteration: 1100; Train Loss: 5.1783 | Perplexity: 177.3822\n",
            "Iteration: 1120; Train Loss: 5.0127 | Perplexity: 150.3164\n",
            "Iteration: 1140; Train Loss: 5.1043 | Perplexity: 164.7272\n",
            "Iteration: 1160; Train Loss: 5.1592 | Perplexity: 174.0243\n",
            "Iteration: 1180; Train Loss: 4.9149 | Perplexity: 136.3103\n",
            "Iteration: 1200; Train Loss: 5.1555 | Perplexity: 173.3788\n",
            "Iteration: 1220; Train Loss: 4.8897 | Perplexity: 132.9090\n",
            "Iteration: 1240; Train Loss: 5.2172 | Perplexity: 184.4216\n",
            "Iteration: 1260; Train Loss: 5.0424 | Perplexity: 154.8461\n",
            "Iteration: 1280; Train Loss: 5.0194 | Perplexity: 151.3146\n",
            "Iteration: 1300; Train Loss: 5.1056 | Perplexity: 164.9482\n",
            "Iteration: 1320; Train Loss: 4.8113 | Perplexity: 122.8908\n",
            "Iteration: 1340; Train Loss: 4.8114 | Perplexity: 122.9008\n",
            "Iteration: 1360; Train Loss: 4.7711 | Perplexity: 118.0484\n",
            "Iteration: 1380; Train Loss: 4.9866 | Perplexity: 146.4423\n",
            "Iteration: 1400; Train Loss: 4.8832 | Perplexity: 132.0535\n",
            "Iteration: 1420; Train Loss: 4.8235 | Perplexity: 124.4033\n",
            "Iteration: 1440; Train Loss: 4.7640 | Perplexity: 117.2097\n",
            "Iteration: 1460; Train Loss: 4.6560 | Perplexity: 105.2189\n",
            "Iteration: 1480; Train Loss: 4.4746 | Perplexity: 87.7633\n",
            "Iteration: 1500; Train Loss: 4.9647 | Perplexity: 143.2676\n",
            "Iteration: 1520; Train Loss: 5.1497 | Perplexity: 172.3726\n",
            "Iteration: 1540; Train Loss: 4.6601 | Perplexity: 105.6424\n",
            "Iteration: 1560; Train Loss: 4.9061 | Perplexity: 135.1124\n",
            "Iteration: 1580; Train Loss: 4.6991 | Perplexity: 109.8441\n",
            "Iteration: 1600; Train Loss: 4.5200 | Perplexity: 91.8370\n",
            "Iteration: 1620; Train Loss: 5.5316 | Perplexity: 252.5514\n",
            "Iteration: 1640; Train Loss: 4.8826 | Perplexity: 131.9707\n",
            "Iteration: 1660; Train Loss: 4.8123 | Perplexity: 123.0199\n",
            "Iteration: 1680; Train Loss: 4.9445 | Perplexity: 140.4005\n",
            "Iteration: 1700; Train Loss: 4.5698 | Perplexity: 96.5221\n",
            "Iteration: 1720; Train Loss: 4.8462 | Perplexity: 127.2529\n",
            "Iteration: 1740; Train Loss: 4.8921 | Perplexity: 133.2320\n",
            "Iteration: 1760; Train Loss: 4.5877 | Perplexity: 98.2648\n",
            "Iteration: 1780; Train Loss: 4.5517 | Perplexity: 94.7890\n",
            "Iteration: 1800; Train Loss: 5.5305 | Perplexity: 252.2592\n",
            "Iteration: 1820; Train Loss: 4.6571 | Perplexity: 105.3332\n",
            "Iteration: 1840; Train Loss: 4.6071 | Perplexity: 100.1887\n",
            "Iteration: 1860; Train Loss: 4.3578 | Perplexity: 78.0817\n",
            "Iteration: 1880; Train Loss: 5.2403 | Perplexity: 188.7310\n",
            "Iteration: 1900; Train Loss: 4.2063 | Perplexity: 67.1069\n",
            "Iteration: 1920; Train Loss: 4.7228 | Perplexity: 112.4802\n",
            "Iteration: 1940; Train Loss: 4.4055 | Perplexity: 81.8961\n",
            "Iteration: 1960; Train Loss: 4.3031 | Perplexity: 73.9254\n",
            "Iteration: 1980; Train Loss: 4.5296 | Perplexity: 92.7256\n",
            "Iteration: 2000; Train Loss: 4.3279 | Perplexity: 75.7838\n",
            "Validation Loss: 4.6835 | Perplexity: 108.1472\n",
            "Iteration: 2020; Train Loss: 4.3447 | Perplexity: 77.0688\n",
            "Iteration: 2040; Train Loss: 4.3257 | Perplexity: 75.6189\n",
            "Iteration: 2060; Train Loss: 4.4208 | Perplexity: 83.1668\n",
            "Iteration: 2080; Train Loss: 4.3494 | Perplexity: 77.4305\n",
            "Iteration: 2100; Train Loss: 4.2747 | Perplexity: 71.8598\n",
            "Iteration: 2120; Train Loss: 4.3591 | Perplexity: 78.1903\n",
            "Iteration: 2140; Train Loss: 4.5682 | Perplexity: 96.3736\n",
            "Iteration: 2160; Train Loss: 4.4954 | Perplexity: 89.6083\n",
            "Iteration: 2180; Train Loss: 4.7300 | Perplexity: 113.2960\n",
            "Iteration: 2200; Train Loss: 4.5311 | Perplexity: 92.8576\n",
            "Iteration: 2220; Train Loss: 4.4810 | Perplexity: 88.3239\n",
            "Iteration: 2240; Train Loss: 4.1265 | Perplexity: 61.9581\n",
            "Iteration: 2260; Train Loss: 4.5121 | Perplexity: 91.1130\n",
            "Iteration: 2280; Train Loss: 4.5119 | Perplexity: 91.0927\n",
            "Iteration: 2300; Train Loss: 5.2020 | Perplexity: 181.6378\n",
            "Iteration: 2320; Train Loss: 4.2662 | Perplexity: 71.2472\n",
            "Iteration: 2340; Train Loss: 4.6039 | Perplexity: 99.8763\n",
            "Iteration: 2360; Train Loss: 4.1312 | Perplexity: 62.2546\n",
            "Iteration: 2380; Train Loss: 4.4736 | Perplexity: 87.6743\n",
            "Iteration: 2400; Train Loss: 4.6848 | Perplexity: 108.2838\n",
            "Iteration: 2420; Train Loss: 4.3574 | Perplexity: 78.0525\n",
            "Iteration: 2440; Train Loss: 4.2750 | Perplexity: 71.8818\n",
            "Iteration: 2460; Train Loss: 4.2017 | Perplexity: 66.8004\n",
            "Iteration: 2480; Train Loss: 4.3026 | Perplexity: 73.8937\n",
            "Iteration: 2500; Train Loss: 4.7273 | Perplexity: 112.9942\n",
            "Iteration: 2520; Train Loss: 4.0405 | Perplexity: 56.8560\n",
            "Iteration: 2540; Train Loss: 4.6256 | Perplexity: 102.0652\n",
            "Iteration: 2560; Train Loss: 4.1851 | Perplexity: 65.6986\n",
            "Iteration: 2580; Train Loss: 4.2031 | Perplexity: 66.8938\n",
            "Iteration: 2600; Train Loss: 3.9702 | Perplexity: 52.9974\n",
            "Iteration: 2620; Train Loss: 4.0224 | Perplexity: 55.8359\n",
            "Iteration: 2640; Train Loss: 4.4680 | Perplexity: 87.1824\n",
            "Iteration: 2660; Train Loss: 4.0799 | Perplexity: 59.1413\n",
            "Iteration: 2680; Train Loss: 4.1586 | Perplexity: 63.9822\n",
            "Iteration: 2700; Train Loss: 4.4063 | Perplexity: 81.9695\n",
            "Iteration: 2720; Train Loss: 4.2284 | Perplexity: 68.6095\n",
            "Iteration: 2740; Train Loss: 4.2057 | Perplexity: 67.0696\n",
            "Iteration: 2760; Train Loss: 4.2714 | Perplexity: 71.6197\n",
            "Iteration: 2780; Train Loss: 4.2511 | Perplexity: 70.1810\n",
            "Iteration: 2800; Train Loss: 4.7536 | Perplexity: 115.9998\n",
            "Iteration: 2820; Train Loss: 3.9351 | Perplexity: 51.1658\n",
            "Iteration: 2840; Train Loss: 4.1927 | Perplexity: 66.1991\n",
            "Iteration: 2860; Train Loss: 4.1261 | Perplexity: 61.9382\n",
            "Iteration: 2880; Train Loss: 3.7806 | Perplexity: 43.8437\n",
            "Iteration: 2900; Train Loss: 4.4101 | Perplexity: 82.2788\n",
            "Iteration: 2920; Train Loss: 4.3148 | Perplexity: 74.7954\n",
            "Iteration: 2940; Train Loss: 4.5893 | Perplexity: 98.4296\n",
            "Iteration: 2960; Train Loss: 4.0328 | Perplexity: 56.4193\n",
            "Iteration: 2980; Train Loss: 4.1931 | Perplexity: 66.2254\n",
            "Iteration: 3000; Train Loss: 3.8473 | Perplexity: 46.8644\n",
            "Validation Loss: 4.3332 | Perplexity: 76.1844\n",
            "Iteration: 3020; Train Loss: 4.0029 | Perplexity: 54.7580\n",
            "Iteration: 3040; Train Loss: 3.9824 | Perplexity: 53.6433\n",
            "Iteration: 3060; Train Loss: 3.9214 | Perplexity: 50.4691\n",
            "Iteration: 3080; Train Loss: 4.3020 | Perplexity: 73.8453\n",
            "Iteration: 3100; Train Loss: 4.0182 | Perplexity: 55.6013\n",
            "Iteration: 3120; Train Loss: 3.8764 | Perplexity: 48.2516\n",
            "Iteration: 3140; Train Loss: 4.4955 | Perplexity: 89.6166\n",
            "Iteration: 3160; Train Loss: 4.2358 | Perplexity: 69.1164\n",
            "Iteration: 3180; Train Loss: 3.8506 | Perplexity: 47.0234\n",
            "Iteration: 3200; Train Loss: 3.8806 | Perplexity: 48.4555\n",
            "Iteration: 3220; Train Loss: 3.9630 | Perplexity: 52.6155\n",
            "Iteration: 3240; Train Loss: 3.6532 | Perplexity: 38.5999\n",
            "Iteration: 3260; Train Loss: 3.9777 | Perplexity: 53.3924\n",
            "Iteration: 3280; Train Loss: 4.4884 | Perplexity: 88.9788\n",
            "Iteration: 3300; Train Loss: 4.8873 | Perplexity: 132.5987\n",
            "Iteration: 3320; Train Loss: 3.8058 | Perplexity: 44.9629\n",
            "Iteration: 3340; Train Loss: 3.7705 | Perplexity: 43.4005\n",
            "Iteration: 3360; Train Loss: 3.9821 | Perplexity: 53.6306\n",
            "Iteration: 3380; Train Loss: 3.9112 | Perplexity: 49.9595\n",
            "Iteration: 3400; Train Loss: 4.0485 | Perplexity: 57.3137\n",
            "Iteration: 3420; Train Loss: 3.9104 | Perplexity: 49.9212\n",
            "Iteration: 3440; Train Loss: 4.5572 | Perplexity: 95.3156\n",
            "Iteration: 3460; Train Loss: 3.7373 | Perplexity: 41.9841\n",
            "Iteration: 3480; Train Loss: 3.4991 | Perplexity: 33.0853\n",
            "Iteration: 3500; Train Loss: 3.7431 | Perplexity: 42.2267\n",
            "Iteration: 3520; Train Loss: 3.6309 | Perplexity: 37.7469\n",
            "Iteration: 3540; Train Loss: 3.8857 | Perplexity: 48.7032\n",
            "Iteration: 3560; Train Loss: 3.7789 | Perplexity: 43.7700\n",
            "Iteration: 3580; Train Loss: 3.7117 | Perplexity: 40.9251\n",
            "Iteration: 3600; Train Loss: 4.1489 | Perplexity: 63.3662\n",
            "Iteration: 3620; Train Loss: 4.6093 | Perplexity: 100.4169\n",
            "Iteration: 3640; Train Loss: 4.0046 | Perplexity: 54.8493\n",
            "Iteration: 3660; Train Loss: 3.8784 | Perplexity: 48.3466\n",
            "Iteration: 3680; Train Loss: 3.5964 | Perplexity: 36.4658\n",
            "Iteration: 3700; Train Loss: 3.7290 | Perplexity: 41.6375\n",
            "Iteration: 3720; Train Loss: 3.8075 | Perplexity: 45.0364\n",
            "Iteration: 3740; Train Loss: 3.8958 | Perplexity: 49.1965\n",
            "Iteration: 3760; Train Loss: 3.8658 | Perplexity: 47.7405\n",
            "Iteration: 3780; Train Loss: 3.6344 | Perplexity: 37.8786\n",
            "Iteration: 3800; Train Loss: 3.5617 | Perplexity: 35.2225\n",
            "Iteration: 3820; Train Loss: 4.4001 | Perplexity: 81.4587\n",
            "Iteration: 3840; Train Loss: 3.6767 | Perplexity: 39.5172\n",
            "Iteration: 3860; Train Loss: 3.7107 | Perplexity: 40.8815\n",
            "Iteration: 3880; Train Loss: 3.6705 | Perplexity: 39.2698\n",
            "Iteration: 3900; Train Loss: 3.3553 | Perplexity: 28.6551\n",
            "Iteration: 3920; Train Loss: 3.4400 | Perplexity: 31.1879\n",
            "Iteration: 3940; Train Loss: 4.7592 | Perplexity: 116.6472\n",
            "Iteration: 3960; Train Loss: 3.5198 | Perplexity: 33.7773\n",
            "Iteration: 3980; Train Loss: 3.7111 | Perplexity: 40.9004\n",
            "Iteration: 4000; Train Loss: 3.4197 | Perplexity: 30.5612\n",
            "Validation Loss: 4.0512 | Perplexity: 57.4671\n",
            "Final checkpoint saved to W&B.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Free unused memory from GPU\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Optional: clear any collected garbage (in case you deleted variables)\n",
        "import gc\n",
        "gc.collect()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N_xlFHUGhqFw",
        "outputId": "91778a4f-f197-48e1-9a46-b06b156f342d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4956"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "#from utils import indexesFromSentence, normalizeString\n",
        "#from config import MAX_LENGTH\n",
        "\n",
        "# Helper to move tensor to correct device\n",
        "def to_device(tensor):\n",
        "    return tensor.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
        "\n",
        "# Safe version that logs unknown words\n",
        "def safe_indexesFromSentence(voc, sentence):\n",
        "    missing = []\n",
        "    indexes = []\n",
        "    for word in sentence.split(\" \"):\n",
        "        if word in voc.word2index:\n",
        "            indexes.append(voc.word2index[word])\n",
        "        else:\n",
        "            missing.append(word)\n",
        "    if missing:\n",
        "        print(f\"Missing words in vocab: {missing}\")\n",
        "    return indexes + [2]  # EOS_token\n",
        "\n",
        "# Greedy decoder\n",
        "class GreedySearchDecoder(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(GreedySearchDecoder, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, input_seq, input_length, max_length):\n",
        "        encoder_outputs, encoder_hidden = self.encoder(input_seq, input_length)\n",
        "\n",
        "        # For LSTM: encoder_hidden is a tuple (h, c)\n",
        "        decoder_hidden = (encoder_hidden[0][:self.decoder.n_layers], encoder_hidden[1][:self.decoder.n_layers])\n",
        "        decoder_input = torch.ones(1, 1, device=input_seq.device, dtype=torch.long) * 1  # SOS_token\n",
        "\n",
        "        all_tokens = torch.zeros([0], device=input_seq.device, dtype=torch.long)\n",
        "        all_scores = torch.zeros([0], device=input_seq.device)\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_scores, decoder_input = torch.max(decoder_output, dim=1)\n",
        "            all_tokens = torch.cat((all_tokens, decoder_input), dim=0)\n",
        "            all_scores = torch.cat((all_scores, decoder_scores), dim=0)\n",
        "            decoder_input = torch.unsqueeze(decoder_input, 0)\n",
        "\n",
        "        return all_tokens, all_scores\n",
        "\n",
        "# Final evaluation wrapper\n",
        "def evaluate(encoder, decoder, searcher, voc, sentence, max_length=MAX_LENGTH):\n",
        "    sentence = normalizeString(sentence)\n",
        "    indexes_batch = [safe_indexesFromSentence(voc, sentence)]\n",
        "    lengths = torch.tensor([len(indexes) for indexes in indexes_batch])\n",
        "    input_batch = torch.LongTensor(indexes_batch).transpose(0, 1)\n",
        "    input_batch = to_device(input_batch)\n",
        "    lengths = lengths.to(\"cpu\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        tokens, scores = searcher(input_batch, lengths, max_length)\n",
        "    decoded_words = [voc.index2word[token.item()] for token in tokens]\n",
        "    return decoded_words"
      ],
      "metadata": {
        "id": "V9VLnR_An44-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "artifact = wandb.use_artifact(\"abhi1199-city-university-of-london/seq2seqLSTM_last/final_chatbot_checkpoint:v1\", type=\"model\")\n",
        "artifact_dir = artifact.download()\n",
        "\n",
        "checkpoint_path = os.path.join(artifact_dir, \"final_checkpoint.tar\")\n",
        "voc_path = os.path.join(artifact_dir, \"voc.pkl\")\n",
        "\n",
        "\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "with open(voc_path, \"rb\") as f:\n",
        "    voc = pickle.load(f)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TCOfvPFLXg9-",
        "outputId": "ee2c2bf5-7498-474c-e9bb-b1fb82b1f278"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact final_chatbot_checkpoint:v1, 693.43MB. 2 files... \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   2 of 2 files downloaded.  \n",
            "Done. 0:0:5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "embedding = load_glove_embeddings(\n",
        "    voc,\n",
        "    glove_path=\"/content/glove.6B.300d.txt\",\n",
        "    embedding_dim=300,\n",
        "    freeze=False\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "encoder = LSTMEncoder(\n",
        "    hidden_size=1000,\n",
        "    embedding=embedding,\n",
        "    n_layers=2,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "decoder = LSTMAttnDecoder(\n",
        "    attn_model=\"dot\",\n",
        "    embedding=embedding,\n",
        "    hidden_size=1000,\n",
        "    output_size=voc.num_words,\n",
        "    n_layers=2,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "\n",
        "encoder.load_state_dict(checkpoint[\"encoder_state\"])\n",
        "decoder.load_state_dict(checkpoint[\"decoder_state\"])\n",
        "embedding.load_state_dict(checkpoint[\"embedding_state\"])\n"
      ],
      "metadata": {
        "id": "dW8xfNbZX2Wr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd78c85a-2df1-4dec-ff9d-9f6a6f68a890"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading GloVe embeddings...\n",
            "Found 9680/9891 words in GloVe.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#from evaluate import GreedySearchDecoder, evaluate\n",
        "\n",
        "encoder.eval()\n",
        "decoder.eval()\n",
        "searcher = GreedySearchDecoder(encoder, decoder)\n"
      ],
      "metadata": {
        "id": "OR1Hc7HDOhbi"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat():\n",
        "    print(\"FinanceBot is ready! Type 'quit' to exit.\")\n",
        "    while True:\n",
        "        input_sentence = input(\"> \")\n",
        "        if input_sentence.lower() in [\"quit\", \"q\"]:\n",
        "            break\n",
        "        try:\n",
        "            output_words = evaluate(encoder, decoder, searcher, voc, input_sentence)\n",
        "            output_words = [w for w in output_words if w not in [\"EOS\", \"PAD\"]]\n",
        "            print(\"Bot:\", \" \".join(output_words))\n",
        "        except KeyError:\n",
        "            print(\"Oops! Encountered unknown word.\")\n",
        "\n",
        "chat()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "seakwdpe7lfK",
        "outputId": "b879b141-99dd-4f8b-be74-03ffa6e16f82"
      },
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "FinanceBot is ready! Type 'quit' to exit.\n",
            "> hey\n",
            "Bot:  .\n",
            "> hello\n",
            "Bot: def reversestring\n",
            "> car loan\n",
            "Bot: the total price is . . . . . .\n",
            "> what is car loan\n",
            "Bot: the car is a car . . . .\n",
            "> i want to buy a car. should i buy or not\n",
            "Bot: i am to buy to buy a store . . .\n",
            "> What percent of my salary should I save?\n",
            "Bot: the sentence of the sentence is positive . . .\n",
            "> Is it wise to switch investment strategy frequently?\n",
            "Bot: . is a goals to a to to . . . . . .\n",
            "> How to motivate young people to save money\n",
            "Bot: . are a few to to to to to their . .\n",
            "> Is it ever a good idea to close credit cards?\n",
            "Bot: this is a customer service . . . . .\n",
            ">  Will I be paid dividends if I own shares?\n",
            "Bot: i am t i don t you you . .\n",
            "> quit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "N1anT_tUqobe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "k3Np8s45Y1oB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " # i want to buy a car. should i buy or not\n",
        "\n",
        " # What percent of my salary should I save?\n",
        "\n",
        " # Is it wise to switch investment strategy frequently?\n",
        "\n",
        " # The best investment at this stage is a good, easy to understand but thorough book on finance\n",
        "\n",
        " # How to motivate young people to save money\n",
        "\n",
        " # How much should a new graduate with new job put towards a car?\n",
        "\n",
        " # What are my investment options in real estate?\n",
        "\n",
        " # Is it ever a good idea to close credit cards?\n",
        "\n",
        "\n",
        " # Would I need to keep track of 1099s?\n",
        "\n",
        " # Will I be paid dividends if I own shares?\n"
      ],
      "metadata": {
        "id": "4eNAfbacYu_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "w6pMoSDGSvue"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}